from transformers import AutoTokenizer, DebertaV2Model
import torch
import torch.nn.functional as F
from kiwipiepy import Kiwi
from tqdm import tqdm
from train import Config, Variables, TaggingModel, LabelData
from typing import Literal
import numpy as np
import json
from dataclasses import dataclass
import os

@dataclass
class FileNames():
    clause_model_pt : str = "clause_model_earth.pt"
    splited_json : str    = 'splited.json'
    embedding_np : str    = 'clause_embedding.npy'
    significant_json: str = 'significant.jsonl'
    saved_temp_dir : str  = './saved_temp'

@torch.no_grad()
def prediction(model, tokenizer, sentence, label_map, device='cuda', max_length=128, return_cls=False):
    """
    ì£¼ì–´ì§„ ë¬¸ì¥ì— ëŒ€í•´ í† í° ë‹¨ìœ„ ë¶„ë¥˜ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    - ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  ì…ë ¥ í† í°ì„ êµ¬ì„±
    - ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ IDë¥¼ ë¼ë²¨ë¡œ ë³€í™˜í•˜ì—¬ confidenceì™€ í•¨ê»˜ ë°˜í™˜

    Args:
        model: í•™ìŠµëœ tagging model
        tokenizer: ì‚¬ì „ í•™ìŠµëœ í† í¬ë‚˜ì´ì €
        sentence: ì…ë ¥ ë¬¸ì¥
        label_map: ì˜ˆì¸¡ ID â†’ ë¼ë²¨ ë§¤í•‘ dict
        device: ì—°ì‚° ì¥ì¹˜ (ê¸°ë³¸ê°’ 'cuda')
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        return_cls: [CLS] ì„ë² ë”© ë²¡í„° ë°˜í™˜ ì—¬ë¶€

    Returns:
        List of (token, label, confidence) ë˜ëŠ” (ìœ„ ê²°ê³¼, cls_vector)
    """
    model.eval()
    model.to(device)

    encoding = tokenizer(
        sentence,
        return_tensors='pt',
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_offsets_mapping=True,
        return_attention_mask=True
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    offset_mapping = encoding['offset_mapping'][0]  # (L, 2)

    outputs, cls_vector = model({'input_ids': input_ids, 'attention_mask': attention_mask}, return_cls=True)
    confidences = [float(int(float(max(m)) * 10000) / 10000) for m in outputs[0]]  # ê° í† í°ì˜ confidence ì •ê·œí™”
    preds = torch.argmax(outputs, dim=-1)[0].cpu().tolist()  # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    results = []
    for i, (token, pred, confidence, offset) in enumerate(zip(tokens, preds, confidences, offset_mapping)):
        if offset[0].item() == 0 and offset[1].item() == 0:
            continue  # PAD í† í° ì œì™¸
        results.append((token, label_map[pred], confidence))

    return (results, cls_vector) if return_cls else results

def recover_wordpieces(tokens: list) -> str:
    """
    WordPiece í† í°ë“¤ì„ ì›ë˜ ë‹¨ì–´ë¡œ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    - '##'ë¡œ ì‹œì‘í•˜ëŠ” í† í°ì€ ì´ì „ í† í°ê³¼ ì—°ê²°í•˜ì—¬ ë‹¨ì–´ ë³µì›

    Args:
        tokens: WordPiece í† í° ë¦¬ìŠ¤íŠ¸

    Returns:
        ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©ëœ ë¬¸ìì—´
    """
    words = []
    current_word = ''
    for token in tokens:
        if token.startswith('##'):
            current_word += token[2:]
        else:
            if current_word:
                words.append(current_word)
            current_word = token
    if current_word:
        words.append(current_word)
    return ' '.join(words)

def highlight(sentences: list[list[str]], highlight_words: list[list[list[str]]]) -> str:
    """
    ê°•ì¡° ë‹¨ì–´ê°€ í¬í•¨ëœ êµ¬ë¬¸ì„ ANSI ìƒ‰ìƒìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ (ì½˜ì†” ê¸°ë°˜ ì‹œê°í™”)
    - ê° ì ˆ ë‹¨ìœ„ë¡œ í•˜ì´ë¼ì´íŠ¸ ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš° ìƒ‰ìƒ ì ìš©

    Args:
        sentences: ì ˆ ë¦¬ìŠ¤íŠ¸ (ex: [['êµ¬ë¬¸1', 'êµ¬ë¬¸2'], [...], ...])
        highlight_words: ê°•ì¡°í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ì ˆë³„ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸)

    Returns:
        ê°•ì¡°ëœ ë¬¸ì¥ ë¬¸ìì—´ (ì¤„ë°”ê¿ˆìœ¼ë¡œ ì´ì–´ì§)
    """
    color = ('\033[95m', '\033[0m')  # ANSI ì½˜ì†” ë§ˆì  íƒ€
    highlighted_sentences = []

    def split_by_keyword(text: str, keyword: str):
        idx = text.find(keyword)
        return [text[:idx], keyword, text[idx + len(keyword):]] if idx != -1 else []

    for clause_list, clause_keywords in zip(sentences, highlight_words):
        highlighted_clauses = []
        for clause, keywords in zip(clause_list, clause_keywords):
            result = []
            for word in clause.split():
                q = sum([split_by_keyword(word, term) for term in keywords], [])
                if q:
                    result.append(f"{q[0]}{color[0]}{q[1]}{color[1]}{q[2]}")
                else:
                    result.append(word)
            highlighted_clauses.append(' '.join(result))
        highlighted_sentence = ' / '.join(highlighted_clauses)
        highlighted_sentences.append(highlighted_sentence)

    return '\n'.join(highlighted_sentences)

def highlight_jsonl(jsonl_path: str):
    """
    JSONL íŒŒì¼ì„ ì½ì–´ ìƒ‰ìƒ ê°•ì¡°ëœ ë¬¸ì¥ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    - ê° ë¼ì¸ì€ ì ˆ ë¦¬ìŠ¤íŠ¸ì™€ ê°•ì¡° ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì•¼ í•¨

    Args:
        jsonl_path: JSONL íŒŒì¼ ê²½ë¡œ

    Returns:
        ê°•ì¡°ëœ ë¬¸ì¥ ì¶œë ¥ ë¬¸ìì—´
    """
    sentences, highlight_words = [], []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            sentences.append(item["clause"])
            highlight_words.append(item["highlight"])
    return highlight(sentences, highlight_words)

class ClauseSpliting:
    """
    ì…ë ¥ ë¬¸ì¥ì„ í† í° ë¶„ë¥˜ ëª¨ë¸ë¡œ ì ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³ , ê° ì ˆì— ëŒ€í•´ ì„ë² ë”© ë° í•µì‹¬ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - ë¬¸ì¥ ë‚´ êµ¬ë¬¸ ë¶„ë¦¬ (E/E2/E3 íƒœê·¸ ê¸°ë°˜)
    - ê° êµ¬ë¬¸ë³„ DeBERTa ê¸°ë°˜ [CLS] ì„ë² ë”© ì¶”ì¶œ
    - êµ¬ë¬¸ ì„ë² ë”©ê³¼ í† í° ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ í†µí•´ ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ íƒìƒ‰
    """
    def __init__(self, sentences, config = Config(), filenames =FileNames(), e_option: Literal['all', 'E3', 'E2', 'E'] = 'E3', threshold=True):
        """
        ì´ˆê¸°í™” ë©”ì„œë“œ
        - tokenizer, tagging model, embedding model, ì„¤ì • ê°’ ë“±ì„ ë¡œë“œí•¨
        - ì ˆ ë¶„ë¦¬ ìˆ˜í–‰ ë° ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        - ê° ì ˆì— ëŒ€í•´ ì„ë² ë”© ìˆ˜í–‰ ë° ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ ì¶”ì¶œ
        """
        self.kiwi = Kiwi()
        self.filenames = filenames
        self.config = config
        self.config.save_batch = getattr(self.config, 'save_batch', 100)
        self.config.return_embed_max = getattr(self.config, 'return_embed_max', 200)
        self.config.important_words_ratio = getattr(self.config, 'important_words_ratio', 0.6)
        self.model = TaggingModel(self.config)
        self.model.load_state_dict(torch.load(self.filenames.clause_model_pt))
        self.embedding_model = DebertaV2Model.from_pretrained(self.config.model)
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model)
        self.sentences = sentences
        self.cls_vectors = []
        option_map = {'all': ['E', 'E2', 'E3'], 'E2': ['E', 'E2'], 'E3': ['E', 'E3'], 'E': ['E']}
        self.elist = option_map.get(e_option, ['E'])
        self.threshold = Variables().confidence_avg * self.config.confidence_threshold if threshold else 0.0
        self.splited = self.split2Clause()

        with open(self.filenames.splited_json, "w", encoding="utf-8-sig") as f:
            json.dump(self.splited, f, ensure_ascii=False, indent=2)

        self.embeds = self.clause_embedding(self.splited)

    def split2Clause(self):
        """
        ë¬¸ì¥ì„ tagging ëª¨ë¸ì„ í†µí•´ í† í° ë‹¨ìœ„ë¡œ ë¶„ë¥˜í•˜ê³ , ì„¤ì •ëœ íƒœê·¸(E, E2, E3)ì— ë”°ë¼ ì ˆ ë‹¨ìœ„ë¡œ ë¶„í• 

        Returns:
            ì ˆ ë¦¬ìŠ¤íŠ¸ (str ë‹¨ìœ„ êµ¬ë¬¸ ë¶„ë¦¬ëœ ê²°ê³¼)
        """
        if isinstance(self.sentences, str):
            _sentences = [self.sentences]
        else:
            _sentences = self.sentences

        results = []
        for sentence in tqdm(_sentences):
            predicted = prediction(self.model, self.tokenizer, sentence, LabelData().id2label, return_cls=True)
            self.cls_vectors.append(predicted[-1])  # [CLS] ë²¡í„° ì €ì¥
            predicted = predicted[0]  # ì˜ˆì¸¡ ê²°ê³¼ë§Œ ì¶”ì¶œ

            clauses, clause, switch = [], [], False
            for i, (tok, label, confidence) in enumerate(predicted):
                if label in self.elist and confidence > self.threshold and not self.is_segm(tok, predicted[i][0]):
                    switch = True
                elif switch:
                    recovered = recover_wordpieces(clause)
                    if len(recovered.split()) < 2 and clauses:
                        clauses[-1] += ' ' + recovered.strip()
                    else:
                        clauses.append(recovered)
                    clause, switch = [], False
                clause.append(tok)
            if clause:
                clauses.append(recover_wordpieces(clause))
            results.append(clauses)

        return results if not isinstance(self.sentences, str) else results[0]

    def clause_embedding(self, splited):
        """
        ì ˆ ë‹¨ìœ„ë¡œ DeBERTa [CLS] ì„ë² ë”©ì„ ì¶”ì¶œí•˜ê³ , ê° í† í° ì„ë² ë”©ê³¼ cosine similarityë¥¼ ê³„ì‚°í•´ ì¤‘ìš”í•œ ë‹¨ì–´ ì¶”ì¶œ
        - ì ˆì˜ [CLS] ì„ë² ë”©ì€ tempì— ì €ì¥
        - ê° í† í° ë²¡í„°ì™€ [CLS] ë²¡í„° ê°„ cosine ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ 
          ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ìƒìœ„ 60%ë¥¼ ì¶”ì¶œí•˜ì—¬ highlight ëŒ€ìƒ ì„ ì •

        Returns:
            ì „ì²´ ì ˆì˜ [CLS] ë²¡í„° ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None (ì €ì¥ íŒŒì¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
        """
        def save_batch_npy(batch_result, save_dir, batch_idx):
            os.makedirs(save_dir, exist_ok=True)
            path = os.path.join(save_dir, f'embedding_batch_{batch_idx}.npy')
            np.save(path, np.array(batch_result, dtype=object))

        with open(self.filenames.significant_json, "w", encoding="utf-8") as f:
            pass  # ì´ˆê¸°í™”

        all_result = [] if len(splited) < self.config.return_embed_max else None
        for batch_idx in range(0, len(splited), self.config.save_batch):
            batch = splited[batch_idx:batch_idx + self.config.save_batch]
            result, highlighted = [], []

            for ss in tqdm(batch, desc=f"Batch {batch_idx // self.config.save_batch}"):
                temp, highlight_temp = [], []
                for s in ss:
                    # êµ¬ë¬¸ ë‹¨ìœ„ [CLS] ì„ë² ë”© ì¶”ì¶œ
                    inputs = self.tokenizer(s, return_tensors='pt', add_special_tokens=True)
                    input_ids = inputs["input_ids"]
                    with torch.no_grad():
                        outputs = self.embedding_model(**inputs)
                    hidden_states, cls_vector = outputs.last_hidden_state, outputs.last_hidden_state[:, 0, :]
                    temp.append(cls_vector.squeeze(0).cpu().numpy())

                    # ë‹¨ì–´ ìˆ˜ì¤€ ì˜ë¯¸ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì „ì²˜ë¦¬
                    real = self.str2real(s, output_str=False)
                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
                    token_map = []
                    for idx, tok in enumerate(tokens):
                        if tok in self.tokenizer.all_special_tokens:
                            continue
                        clean_tok = tok[2:] if tok.startswith("##") else tok
                        for word in real:
                            if clean_tok in word:
                                token_map.append((word, idx))
                                break

                    # ê° ë‹¨ì–´ë³„ í† í° ì¸ë±ìŠ¤ ì§‘ê³„
                    word2indices = {}
                    for word, idx in token_map:
                        word2indices.setdefault(word, []).append(idx)

                    # ê° ë‹¨ì–´ì˜ ë²¡í„°ë“¤ê³¼ [CLS] ë²¡í„° ê°„ cosine similarity í‰ê·  ê³„ì‚°
                    word_scores = []
                    for word, indices in word2indices.items():
                        vecs = torch.stack([hidden_states[0, i] for i in indices])
                        sims = F.cosine_similarity(vecs, cls_vector[0].unsqueeze(0), dim=1)
                        score = self.rms(sims)
                        word_scores.append((word, float(score)))

                    # ìœ ì‚¬ë„ ê¸°ì¤€ ìƒìœ„ 60% ë‹¨ì–´ë¥¼ ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ë¡œ ì„ ì •
                    word_scores_sorted = sorted(word_scores, key=lambda x: x[1], reverse=True)
                    top_n = max(1, int(len(word_scores_sorted) * self.config.important_words_ratio))
                    top_words = {word for word, _ in word_scores_sorted[:top_n]}
                    highlight_temp.append([word for word in real if word in top_words])
                highlighted.append(highlight_temp)
                result.append(temp)

            for clauses, highlights in zip(batch, highlighted):
                item = {"clause": clauses, "highlight": highlights}
                with open(self.filenames.significant_json, "a", encoding="utf-8") as f:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")

            save_batch_npy(result, self.filenames.saved_temp_dir, batch_idx)
            if all_result is not None:
                all_result.extend(result)

        def load_and_merge_npy(save_dir: str, output_path: str):
            files = [f for f in os.listdir(save_dir) if f.startswith("embedding_batch_") and f.endswith(".npy")]
            if not files:
                raise FileNotFoundError("ë³‘í•©í•  .npy íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))
            if len(files) == 1:
                src = os.path.join(save_dir, files[0])
                dst = os.path.join(save_dir, output_path)
                np.save(dst, np.load(src, allow_pickle=True))
                return
            merged = []
            for file in files:
                batch_path = os.path.join(save_dir, file)
                data = np.load(batch_path, allow_pickle=True)
                merged.extend(data)
            np.save(output_path, np.array(merged, dtype=object))

        load_and_merge_npy(self.filenames.saved_temp_dir, self.filenames.embedding_np)
        return all_result

    def is_gram(self, word):
        """ì£¼ì–´ì§„ ë‹¨ì–´ê°€ ì¡°ì‚¬(J), ì–´ë¯¸(E), ì ‘ë¯¸ì‚¬(XS)ì¸ì§€ ì—¬ë¶€ í™•ì¸"""
        t = self.kiwi.tokenize(word)[-1].tag
        return t[0] in ['J', 'E'] or t[:2] == 'XS'

    def is_segm(self, word, prev):
        """ë‘ í† í° ê²°í•© ì‹œ ì˜ë¯¸ ë‹¨ìœ„(N/V/M/XR ë“±)ë¡œ ë¶„ë¦¬ë  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨"""
        combined = prev + word.strip('#') if word.startswith('#') else prev + ' ' + word
        t = self.kiwi.tokenize(combined)[-1].tag
        return t[0] in ['N', 'V', 'M'] or t[:2] == 'XR'

    def rms(self, x: torch.Tensor) -> torch.Tensor:
        """Root Mean Square ì—°ì‚° (ìœ ì‚¬ë„ í‰ê·  ê³„ì‚° ì‹œ í™œìš©)"""
        return torch.sqrt(torch.mean(x ** 2))

    def str2real(self, text, timecat=True, output_str=True):
        """
        í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ìš”ì†Œë§Œ ì¶”ì¶œ
        timecat=Trueì¼ ê²½ìš° ì‹œê°„ ê´€ë ¨ ìˆ«ì ë¬¶ìŒë„ ì²˜ë¦¬í•¨
        """
        tokens = self.kiwi.tokenize(text)
        return ' '.join(self.bereal(tokens, timecat)) if output_str else self.bereal(tokens, timecat)

    def bereal(self, tokens, timecat=True):
        """
        í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” í˜•íƒœì†Œë§Œ í•„í„°ë§
        - ëª…ì‚¬, ë™ì‚¬, ìˆ«ì, ê´€í˜•ì‚¬ ë“± take ë¦¬ìŠ¤íŠ¸ ì¤‘ì‹¬
        - ì‹œê°„ ì •ë³´ëŠ” ë¶™ì—¬ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        """
        real, timeset = [], []
        take = ['NNG', 'NNP', 'NNB', 'NP', 'NR', 'XR', 'SN', 'SL', 'VV', 'VA', 'MM', 'MAJ', 'MAG']
        timeTrigger = ['ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ì„¸']
        for token in tokens:
            if token.tag in take:
                if not timecat:
                    real.append(token.form)
                    continue
                if token.tag in ['SN', 'NR']:
                    timeset.append(token.form)
                elif token.form in timeTrigger or token.tag == 'NNB':
                    if timeset:
                        timeset.append(token.form)
                elif len(timeset) > 1:
                    real.append(''.join(timeset))
                    timeset = []
                elif timeset:
                    real.append(timeset[0])
                    timeset = []
                real.append(token.form)
        return real
    
    def summary(self, max_sentence: int = 5):
        """
        ë¶„ë¦¬ëœ ì ˆê³¼ í•´ë‹¹ ê°•ì¡° ë‹¨ì–´, [CLS] ì„ë² ë”© ë²¡í„° ì •ë³´ë¥¼ ìš”ì•½ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
        - ì¤‘ìš” ë‹¨ì–´ ê°•ì¡° ì—¬ë¶€, ì„ë² ë”© ê°œìˆ˜, ë¬¸ì¥/ì ˆ ìˆ˜ ë“± í†µê³„ ì œê³µ
        Args:
            max_sentence: ì¶œë ¥í•  ë¬¸ì¥ ìˆ˜ ì œí•œ (ê¸°ë³¸ê°’ 5)
        """
        try:
            # ë§Œì•½ ë©”ëª¨ë¦¬ì— ì—†ê³ , embedding_np íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œ ì‹œë„
            if not self.embeds:
                if os.path.exists(self.filenames.embedding_np):
                    self.embeds = np.load(self.filenames.embedding_np, allow_pickle=True)
                    print("[INFO] ë¡œë”© ì™„ë£Œ â†’ shape:", self.embeds.shape)
                else:
                    print("[ê²½ê³ ] embedding_np íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì¶œë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
        except Exception as e:
            print("[ì—ëŸ¬] ì„ë² ë”© ë³µì› ì‹¤íŒ¨:", e)
            return

        print("\nğŸ“Œ ì ˆ ë¶„ë¦¬ ë° ê°•ì¡° ë‹¨ì–´ ì˜ˆì‹œ:")
        for i, (clauses, embeds) in enumerate(zip(self.splited, self.embeds)):
            if i >= max_sentence:
                print("... ìƒëµ ...")
                break
            print(f"\nğŸŸ¦ ë¬¸ì¥ {i+1}")
            for j, clause in enumerate(clauses):
                highlight = "[ì¤‘ìš” ë‹¨ì–´ ê°•ì¡°ë¨]" if j < len(embeds) else ""
                print(f"  ì ˆ {j+1}: {clause} {highlight}")
            print(f"  â¤ [CLS ì„ë² ë”© ê°œìˆ˜]: {len(embeds)}")

        print("\nâœ… ì „ì²´ ì ˆ ìˆ˜:", sum(len(s) for s in self.splited))
        print("âœ… ì „ì²´ ë¬¸ì¥ ìˆ˜:", len(self.splited))
        print("âœ… ì ˆë‹¹ í‰ê·  ì„ë² ë”© ë²¡í„° ìˆ˜:", round(sum(len(s) for s in self.embeds) / len(self.embeds), 2))



def main():
    example = 'example2.txt'

    config = Config()
    config.confidence_threshold = 0.15

    with open(example, 'r', encoding='utf-8-sig') as f:
        raw = f.read()
        sentences = [r for r in raw.splitlines()]

    cs = ClauseSpliting(sentences, config= config, e_option= 'E3', threshold= True)
    cs.summary()
    
if __name__ == "__main__":
    main()