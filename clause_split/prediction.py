# ì „ì²´ ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ ëª©ì ì„ ê°–ëŠ”ë‹¤:
# - ì…ë ¥ ë¬¸ì¥ì„ ì ˆ(clause) ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³ 
# - ê° ì ˆì˜ ì˜ë¯¸ ì„ë² ë”© ë²¡í„°ì™€ ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ë©°
# - ì ˆ ê°„ì˜ ì˜ë¯¸ì  ê´€ê³„(triplet)ì„ ì¶”ì¶œí•´ Knowledge Graphë¡œ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ì§€ì›í•¨

# === ì£¼ìš” ëª¨ë“ˆ ===
# - ClauseSpliting : ë¬¸ì¥ -> ì ˆ ë¶„ë¦¬ + ì„ë² ë”© + ê°•ì¡° ë‹¨ì–´ ì¶”ì¶œ + ê´€ê³„ ì¶”ì¶œ
# - ClauseDB       : SQLite DB ê´€ë¦¬ ë° ì„ë² ë”© ì¡°íšŒ
# - prediction     : tagging modelì„ í†µí•´ ë¬¸ì¥ì—ì„œ E/E2/E3 ìœ„ì¹˜ ì˜ˆì¸¡
# - highlight      : ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ë¥¼ ì‹œê°í™” ì¶œë ¥

# ========================
# ì£¼ìš” í´ë˜ìŠ¤: ClauseSpliting
# ========================
# - ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹¤ìŒ ë‹¨ê³„ ì²˜ë¦¬:
#   1. split2Clause      : tagging modelë¡œ ì ˆ ë¶„ë¦¬
#   2. clause_embedding  : ê° ì ˆë§ˆë‹¤ [CLS] ì„ë² ë”© ë° ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
#   3. find_rel          : ì ˆ ê°„ ê´€ê³„ triplet êµ¬ì„±
#   4. set_db            : ì ˆì„ clause_id ê¸°ë°˜ìœ¼ë¡œ DBì— ì €ì¥
#   5. splited_id_mapping: DBì—ì„œ id í¬í•¨ëœ ì ˆ êµ¬ì¡° ë°˜í™˜
#   6. print_triplets    : ê´€ê³„ ì¶œë ¥
#   7. summary           : ì „ì²´ ë¶„ì„ ìš”ì•½ ì¶œë ¥

# ======================
# ì£¼ìš” êµ¬ì¡°
# ======================
# clause_id = V*100000 + S*10 + C
#   - V: video index
#   - S: sentence index within video
#   - C: clause index within sentence

# self.splited : [video][sentence][clause] 
# self.embeds  : [video][sentence][clause][768] -> ê° clauseì˜ CLS ì„ë² ë”© ë²¡í„°
# self.meanpooled_embeds: ê° í† í°ì˜ ì„ë² ë”©ì„ clause ë‹¨ìœ„ë¡œ meaní•œ ì„ë² ë”©

# ======================
# ê´€ê³„ ì¶”ì¶œ ê¸°ì¤€
# ======================
# - ì–´ë¯¸(aumi), ì ‘ì†ì‚¬(conj), ì–´ê°„(augan)ì— ë”°ë¼ ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ì´ë‚˜ ì²˜ìŒì„ ë³´ê³  ë‹¨ì„œë¥¼ ì¶”ì¶œ
# - ì—­í• ë§ˆë‹¤ ìš°ì„ ìˆœìœ„ê°€ ë‹¬ë¼ì„œ ë¨¼ì € ë°œê²¬ë˜ëŠ” ë‹¨ì„œë¥¼ ì‚¬ìš©
# - ê´€ê³„ê°€ ì¤‘ì²©ë˜ëŠ” ê²½ìš°(ì•/ë’¤) max()ë¡œ ë®ì–´ì”€

# ======================
# ì£¼ìš” ì„¤ì •ê°’
# ======================
# config.confidence_threshold  : tagging í™•ë¥  threshold (ì ˆ ë¶„ë¦¬ ë¯¼ê°ë„)
# config.important_words_ratio : cosine similarity ìƒìœ„ ëª‡ % ë‹¨ì–´ë¥¼ ê°•ì¡°ë¡œ ë³¼ ê²ƒì¸ê°€
# config.clause_len_threshold  : ì ˆ ìµœì†Œ ê¸¸ì´ ì œí•œ

# ======================
# ì‹¤í–‰ ì§„ì…ì 
# ======================
# main():
#   - youtube_filtered.jsonì´ ìˆìœ¼ë©´ ì „ì²˜ë¦¬ ìƒëµ
#   - ClauseSpliting ê°ì²´ë¥¼ ìƒì„±í•´ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë¶„ë¦¬, ì„ë² ë”©, ê´€ê³„ ì¶”ì¶œ)
#   - ê²°ê³¼ë¥¼ ìš”ì•½ ë° ì¶œë ¥

# ======================
# ê¸°íƒ€ í•¨ìˆ˜
# ======================
# - highlight_jsonl: ê°•ì¡° ë‹¨ì–´ì™€ ì ˆì„ JSONLì—ì„œ ë¶ˆëŸ¬ì™€ ì½˜ì†”ì— ì»¬ëŸ¬ ì¶œë ¥
# - recover_wordpieces: WordPiece í† í°ì„ ì›ë˜ ë‹¨ì–´ë¡œ ë³µì›
# - get_shape: nested list / tensorì˜ shape í™•ì¸ìš© ë””ë²„ê¹… í•¨ìˆ˜
# - bereal: í˜•íƒœì†Œ ë¶„ì„ í›„ ì˜ë¯¸ ìš”ì†Œë§Œ í•„í„°ë§ (ìˆ«ì, ëª…ì‚¬, ë™ì‚¬ ë“±)

# ======================


from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn
import torch.nn.functional as F
from kiwipiepy import Kiwi
from tqdm import tqdm
import numpy as np
import pandas as pd
from train import Config, Variables, TaggingModel, LabelData
from processing import open_and_preprocess, select_terms
from typing import Literal
import json
from dataclasses import dataclass
import os
import sqlite3


@dataclass
class FileNames():
    clause_model_pt : str = "../clause_model_earth.pt"
    # --------------- #
    extra_name : str      = "_top6"
    saved_dir : str       = "./saved_data/"
    splited_json : str    = saved_dir+ f'splited{extra_name}.json'
    embedding_np : str    = saved_dir+ f'clause_embedding{extra_name}.npy'
    sbert_np : str        = saved_dir+ f'sbert{extra_name}.npy'
    significant_jsonl:str = saved_dir+ f'significant{extra_name}.jsonl'
    clause_db: str        = saved_dir+ f'clause{extra_name}.db'
    triplets_np: str      = saved_dir+ f'triplets{extra_name}.npy'
    saved_temp_dir : str  = './saved_temp'
    relation_trigger: str = "../data/relation_trigger.csv"

@torch.no_grad()
def prediction(model, tokenizer, sentence, label_map, device='cuda', max_length=128, return_cls=False, return_lhs = False):
    """
    ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ tagging modelì„ í†µí•´ ê° í† í°ì˜ labelê³¼ confidenceë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

    Args:
        model (nn.Module): í•™ìŠµëœ ì‹œí€€ìŠ¤ íƒœê¹… ëª¨ë¸
        tokenizer (PreTrainedTokenizer): í•´ë‹¹ ëª¨ë¸ì— ë§ëŠ” tokenizer
        sentence (str): ì…ë ¥ ë¬¸ì¥
        label_map (dict): ì˜ˆì¸¡ ê²°ê³¼ IDë¥¼ ë¼ë²¨(str)ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ë§¤í•‘
        device (str): ì—°ì‚°ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        max_length (int): ì…ë ¥ ë¬¸ì¥ì˜ ìµœëŒ€ í† í° ê¸¸ì´
        return_cls (bool): [CLS] ë²¡í„°ë¥¼ ë°˜í™˜í• ì§€ ì—¬ë¶€
        return_lhs (bool): ë§ˆì§€ë§‰ hidden state ì „ì²´ë¥¼ ë°˜í™˜í• ì§€ ì—¬ë¶€

    Returns:
        - return_cls=False: List[Tuple[token, label, confidence]]
        - return_cls=True: Tuple[ìœ„ ë¦¬ìŠ¤íŠ¸, cls_vector] ë˜ëŠ” [ì¶”ê°€ë¡œ hidden state] í¬í•¨
    """

    # gpuì— ëª¨ë¸ì´ ì—†ë‹¤ë©´ ì˜¬ë¦¬ê¸° 
    if next(model.parameters()).device != device:
        model.to(device)
    model.eval()

    encoding = tokenizer(
        sentence,
        return_tensors='pt',
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_offsets_mapping=True,
        return_attention_mask=True
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    offset_mapping = encoding['offset_mapping'][0]  # (L, 2)

    outputs, cls_vector, last_hidden_state= model({'input_ids': input_ids, 'attention_mask': attention_mask}, return_cls=True, return_last_hidden_state=True)
    confidences = [float(int(float(max(m)) * 10000) / 10000) for m in outputs[0]]  # ê° í† í°ì˜ confidence ì •ê·œí™”
    preds = torch.argmax(outputs, dim=-1)[0].cpu().tolist()  # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡
    
    valid_len = attention_mask.sum().item()  # ì‹¤ì œ í† í° ìˆ˜
    masked_lhs = last_hidden_state.squeeze(0)[:valid_len]  # shape: [S, H]

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    be_clause = []
    results = []
    for token, pred, confidence, offset in zip(tokens, preds, confidences, offset_mapping):
        if offset[0].item() == 0 and offset[1].item() == 0:
            continue  # [PAD] í† í° ì œì™¸
        be_clause.append((token, label_map[pred], confidence))
    if return_cls:
        results.append(be_clause)
        results.append(cls_vector.detach().cpu())
    if return_lhs:
        results.append(masked_lhs.detach().cpu())
        
    return tuple(results) if return_cls else be_clause

def recover_wordpieces(tokens: list) -> str:
    """
    WordPiece í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ì›ë˜ ë‹¨ì–´ë¡œ ë³µì›í•©ë‹ˆë‹¤.

    Args:
        tokens (List[str]): WordPieceë¡œ ë¶„í• ëœ í† í° ë¦¬ìŠ¤íŠ¸

    Returns:
        str: ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©ëœ ë¬¸ìì—´ (ì˜ˆ: ['êµ­', '##ë¯¼', 'ì€í–‰'] â†’ 'êµ­ë¯¼ ì€í–‰')
    """
    words = []
    current_word = ''
    for token in tokens:
        if token.startswith('##'):
            current_word += token[2:]
        else:
            if current_word:
                words.append(current_word)
            current_word = token
    if current_word:
        words.append(current_word)
    return ' '.join(words)

def highlight(sentences: list[list[str]], highlight_words: list[list[list[str]]],return_in_list = False) -> str:
    """
    ì ˆê³¼ ê°•ì¡° ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, íŠ¹ì • ë‹¨ì–´ë¥¼ ANSI ìƒ‰ìƒìœ¼ë¡œ ê°•ì¡°í•œ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        sentences (List[List[str]]): ì ˆ ë¦¬ìŠ¤íŠ¸, ex) [[ì ˆ1, ì ˆ2], ...]
        highlight_words (List[List[List[str]]]): ê°•ì¡°í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ê° ì ˆ ë‹¨ìœ„)
        return_in_list (bool): Trueì´ë©´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜, Falseì´ë©´ ì¤„ë°”ê¿ˆ ë¬¸ìì—´ ë°˜í™˜

    Returns:
        str or List[str]: ê°•ì¡° ë‹¨ì–´ê°€ ìƒ‰ì¹ ëœ ë¬¸ì¥
    """

    # print("highlight : ",get_shape(sentences), get_shape(highlight_words))

    color = ('\033[95m', '\033[0m')  # ANSI ì½˜ì†” ë§ˆì  íƒ€
    highlighted_sentences = []

    def split_by_keyword(text: str, keyword: str):
        idx = text.find(keyword)
        return [text[:idx], keyword, text[idx + len(keyword):]] if idx != -1 else []

    for clause_list, clause_keywords in zip(sentences, highlight_words):
        highlighted_clauses = []
        for clause, keywords in zip(clause_list, clause_keywords):
            result = []
            for word in clause.split():
                q = sum([split_by_keyword(word, term) for term in keywords], [])
                if q and type(q)==list:
                    result.append(f"{q[0]}{color[0]}{q[1]}{color[1]}{q[2]}")
                else:
                    result.append(word)
            highlighted_clauses.append(' '.join(result))
        highlighted_sentence = ' / '.join(highlighted_clauses)
        highlighted_sentences.append(highlighted_sentence)

    return highlighted_sentences if return_in_list else '\n'.join(highlighted_sentences)

def highlight_jsonl(jsonl_path: str, sample: int =float('inf'))->list:
    """
    ê°•ì¡° ë‹¨ì–´ê°€ í¬í•¨ëœ JSONL íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ì½˜ì†”ìš© ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        jsonl_path (str): JSONL íŒŒì¼ ê²½ë¡œ
        sample (int): ìµœëŒ€ ì¶œë ¥í•  ë¬¸ì¥ ìˆ˜

    Returns:
        List[str]: ê°•ì¡° ë¬¸ì¥ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """
    videos, highlight_words = [], []
    with open(jsonl_path, "r", encoding="utf-8-sig") as f:
        for i, line in enumerate(f):
            item = json.loads(line)
            videos.append(item["clause"])
            highlight_words.append(item["highlight"])
            if i > sample:
                break
    return [highlight(sents, highlight_sents) for sents, highlight_sents in zip(videos, highlight_words)]

def get_shape(obj):
    """
    ì…ë ¥ ê°ì²´ì˜ ì¬ê·€ì  shape ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        obj (Union[list, np.ndarray, torch.Tensor]): ëŒ€ìƒ ê°ì²´

    Returns:
        Tuple[int, ...]: ê°ì²´ì˜ ë‹¤ì°¨ì› shape
    """
    # í…ì„œë‚˜ ë„˜íŒŒì´ ë°°ì—´ì´ë©´ ë°”ë¡œ shape ë°˜í™˜
    if isinstance(obj, (torch.Tensor, np.ndarray)):
        return tuple(obj.shape)

    # ê¸°ë³¸í˜• (ìˆ«ì, ë¬¸ìì—´ ë“±) -> shape ì—†ìŒ
    if not isinstance(obj, list):
        return ()

    # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
    if len(obj) == 0:
        return (0,)

    # ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ì¬ê·€
    first_shape = get_shape(obj[0])
    return (len(obj),) + first_shape


class ClauseSpliting:
    """
    ë¬¸ì¥ì„ ì ˆ(clause) ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³  ê° ì ˆì˜ ì˜ë¯¸ ì„ë² ë”©, ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´, ê·¸ë¦¬ê³  ì ˆ ê°„ ì˜ë¯¸ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ëŠ” í†µí•© ì²˜ë¦¬ í´ë˜ìŠ¤.

    ì£¼ëœ ê¸°ëŠ¥:
    - êµ¬ë¬¸ ë¶„ë¦¬: ë¬¸ì¥ì„ tagging modelë¡œ ë¶„ì„í•˜ì—¬ E/E2/E3 íƒœê·¸ ê¸°ë°˜ìœ¼ë¡œ ì ˆì„ ë¶„ë¦¬í•¨
    - ì„ë² ë”© ìƒì„±: ê° ì ˆì— ëŒ€í•´ DeBERTa ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ [CLS] ë²¡í„° ë° mean pooling ë²¡í„° ìƒì„±
    - ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ ì¶”ì¶œ: [CLS] ë²¡í„°ì™€ í† í° ê°„ cosine ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”í•œ ë‹¨ì–´ ì„ ì •
    - ê´€ê³„ ì¶”ì¶œ: ë‹¨ì„œì‚¬ì „ ê¸°ë°˜ìœ¼ë¡œ ì ˆ ê°„ ì¸ê³¼/ëŒ€ì¡°/ì¡°ê±´ ë“±ì˜ ì˜ë¯¸ ê´€ê³„(triplet)ë¥¼ ì¶”ì¶œ
    - DB ì €ì¥: ê° ì ˆì„ ê³ ìœ  IDë¡œ êµ¬ì„±ëœ DBì— ì €ì¥í•˜ë©°, ì„ë² ë”©ë„ numpy íŒŒì¼ë¡œ ì €ì¥ ê°€ëŠ¥

    ì£¼ìš” ì…ë ¥:
    - sentences: [str] ë˜ëŠ” [List[List[str]]] í˜•íƒœì˜ ë¬¸ì¥ ì§‘í•©
    - config: ëª¨ë¸ ë° ì„ê³„ê°’ ì„¤ì •ì„ ë‹´ì€ Config ê°ì²´
    - filenames: íŒŒì¼ ê²½ë¡œë¥¼ ë‹´ì€ FileNames ê°ì²´

    ì£¼ìš” ì¶œë ¥/ì €ì¥:
    - self.splited: ì ˆ ë‹¨ìœ„ ë¶„í•  ê²°ê³¼ [[video][sentence][clause]]
    - self.meanpooled_embeds: ì ˆë³„ mean pooling ë²¡í„°
    - self.embeds: DeBERTa ê¸°ë°˜ ì ˆ ì„ë² ë”© ([CLS])
    - ./saved_data ë””ë ‰í† ë¦¬ì— JSON, JSONL, NPY, DB ë“± ë‹¤ìˆ˜ íŒŒì¼ ì €ì¥
    """
    def __init__(self, sentences = None, config = Config(), filenames =FileNames(), e_option: Literal['all', 'E3', 'E2', 'E'] = 'E3', threshold=True, reference_mode = False):
        """
        ì´ˆê¸°í™” ë©”ì„œë“œ
        - tokenizer, tagging model, embedding model, ì„¤ì • ê°’ ë“±ì„ ë¡œë“œí•¨
        - ì ˆ ë¶„ë¦¬ ìˆ˜í–‰ ë° ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        - ê° ì ˆì— ëŒ€í•´ ì„ë² ë”© ìˆ˜í–‰ ë° ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ ì¶”ì¶œ
        """
        self.kiwi = Kiwi()
        self.filenames = filenames
        os.makedirs(filenames.saved_dir, exist_ok=True)
        self.history = {'num_triplets': 0, 'num_clauses': 0, 'num_sentences': 0, 'num_videos': 0}
        self.config = config
        self.config.clause_len_threshold = getattr(self.config, 'clause_len_threshold', 3)
        self.config.save_batch = getattr(self.config, 'save_batch', 100)
        self.config.return_embed_max = getattr(self.config, 'return_embed_max', 200)
        self.config.important_words_ratio = getattr(self.config, 'important_words_ratio', 0.6)
        self.model = TaggingModel(self.config)
        self.model.load_state_dict(torch.load(self.filenames.clause_model_pt))
        self.embedding_model = AutoModel.from_pretrained(self.config.model)
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model)
        self.concat_project = ConcatProject()
        self.sentences = sentences
        self.meanpooled_embeds = None
        self.cls_vectors = []
        self.rel_map = {'ì—†ìŒ': 0, 'ê¸°íƒ€': 1, 'ëŒ€ì¡°/ë³‘ë ¬': 2, 'ìƒí™©': 3, 'ìˆ˜ë‹¨': 4, 'ì—­ì¸ê³¼': 5, 'ì˜ˆì‹œ': 6, 'ì¸ê³¼': 7}
        option_map = {'all': ['E', 'E2', 'E3'], 'E2': ['E', 'E2'], 'E3': ['E', 'E3'], 'E': ['E']}
        self.elist = option_map.get(e_option, ['E'])
        self.threshold = Variables().confidence_avg * self.config.confidence_threshold if threshold else 0.0
        if (not reference_mode) and (not sentences) :
            raise ValueError("sentences must be exist in reference_mode=False")

        if not reference_mode:
            switch = False
            if os.path.exists(self.filenames.splited_json):
                with open(self.filenames.splited_json, "r", encoding="utf-8-sig") as f:
                    self.splited = json.load(f)
                    if (len(self.splited)) != len(self.sentences):
                        switch = True    
            else:
                switch = True
            if switch :
                self.splited, self.meanpooled_embeds = self.split2Clause(self.sentences)
                with open(self.filenames.splited_json, "w", encoding="utf-8-sig") as f:
                    json.dump(self.splited, f, ensure_ascii=False, indent=2)
                self.set_db()
            
            if not os.path.exists(self.filenames.embedding_np):
                self.embeds = self.clause_embedding(self.splited, print_highlighted= False)
            else:
                self.embeds = None

    def make_nd(self, obj: list, target_depth: int = 2):
        """
        ë¦¬ìŠ¤íŠ¸ì˜ í˜„ì¬ ì¤‘ì²© ê¹Šì´ë¥¼ ê³„ì‚°í•˜ê³ , target_depthë§Œí¼ ê°ì‹¸ì„œ ë§ì¶¤.

        Args:
            obj (list): ì…ë ¥ ë¦¬ìŠ¤íŠ¸
            target_depth (int): ëª©í‘œ ì°¨ì› ê¹Šì´ (ì˜ˆ: 2 â†’ 2D, 3 â†’ 3D)

        Returns:
            tuple: (ìˆ˜ì •ëœ ë¦¬ìŠ¤íŠ¸, ì›ë˜ ê¹Šì´)
        """
        depth = 0
        _obj = obj
        while isinstance(_obj, list):
            if not _obj:
                break
            _obj = _obj[0]
            depth += 1

        if depth > target_depth:
            raise ValueError(f"Too much depth ({depth}) for target {target_depth}!")

        for _ in range(target_depth - depth):
            obj = [obj]

        return obj, depth

    def split2Clause(self, data):
        """
        Tagging ëª¨ë¸ì„ í™œìš©í•´ ì…ë ¥ ë¬¸ì¥ì„ ì ˆ(clause) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³ , ê° ì ˆì˜ mean pooling ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.

        ë™ì‘ ë°©ì‹:
        - ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì§•í•˜ì—¬ íƒœê¹… ê²°ê³¼(E/E2/E3)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆì„ ë‚˜ëˆ•ë‹ˆë‹¤.
        - ì ˆì˜ ìµœì†Œ ê¸¸ì´(threshold) ì´í•˜ì¸ ê²½ìš° ì‚­ì œí•©ë‹ˆë‹¤.
        - ê° ì ˆë§ˆë‹¤ hidden stateì—ì„œ mean poolingëœ ì„ë² ë”© ë²¡í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            data (List[str] or List[List[str]]): ë¶„ë¦¬í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ì¥ ê·¸ë£¹

        Returns:
            Tuple[
                List[List[List[str]]],      # ë¶„ë¦¬ëœ ì ˆ ë¦¬ìŠ¤íŠ¸ â†’ [video][sentence][clause]
                List[List[List[Tensor]]]   # ì ˆë³„ mean pooling ì„ë² ë”© ë²¡í„° â†’ [video][sentence][clause][768]
            ]
        """

        total, depth = self.make_nd(data, target_depth=2)
        
        results, embedding_t = [], []  # ìµœì¢… ê²°ê³¼ (ì ˆ ë‹¨ìœ„ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)ì™€ ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        print("split to clause : ")
        for video in tqdm(total):  # ê° 'video' ë‹¨ìœ„ë¡œ ë¬¸ì¥ ê·¸ë£¹ ì²˜ë¦¬ (ì˜ˆ: í•˜ë‚˜ì˜ ë¬¸ì„œë‚˜ ìƒ˜í”Œ)
            video_sents, embedding_v = [], []  # í•´ë‹¹ videoì— ëŒ€í•œ ë¬¸ì¥ ë° ì„ë² ë”© ê²°ê³¼
            cls_temp = []
            for sentence in video:
                # ë¬¸ì¥ì„ ëª¨ë¸ì— ë„£ì–´ ì˜ˆì¸¡ ìˆ˜í–‰
                # return_cls=True: [CLS] ë²¡í„° í¬í•¨, return_lhs=True: hidden state ë°˜í™˜
                predicted = prediction(
                    self.model, self.tokenizer, sentence,
                    LabelData().id2label, return_cls=True, return_lhs=True
                )

                cls_temp.append(predicted[1])  # [CLS] ë²¡í„° ì €ì¥
                embeddings = predicted[2][1:-1]        # [seq_len, hidden_size] í˜•íƒœì˜ ì„ë² ë”©
                predicted = predicted[0]               # í† í° ë¼ë²¨ë§ ê²°ê³¼ [(tok, label, confidence), ...]

                if len(embeddings) != len(predicted):
                    raise ValueError(f"length dismatch :{embeddings}, {len(predicted)}")

                # êµ¬ë¬¸ë“¤, ì„ì‹œêµ¬ë¬¸, êµ¬ë¬¸ë¶„ë¦¬ index, trigger
                clauses, clause, clause_end_idx, switch = [], [], [], False

                for i, (tok, label, confidence) in enumerate(predicted):
                    # íŠ¹ì • ë¼ë²¨(`E`, `E2`, ...)ì´ê³  ì‹ ë¢°ë„ ì„ê³„ê°’ ì´ˆê³¼í•˜ë©° ì˜ë¯¸ì—­ì´ ì•„ë‹Œ ê²½ìš°, ì ˆ ë¶„ë¦¬ ì‹œì‘
                    if label in self.elist and confidence > self.threshold and not self.is_segm(tok, predicted[i][0]):
                        switch = True
                    elif switch:
                        # clause ê²½ê³„ì—ì„œ êµ¬ì ˆ ë³µì›
                        recovered = recover_wordpieces(clause)
                        # ì§§ì€ êµ¬ì ˆì€ ì• êµ¬ì ˆì— ë³‘í•©
                        if len(recovered.split()) < 2 and clauses:
                            clauses[-1] += ' ' + recovered.strip('. ')
                            clause_end_idx[-1] = i
                        else:
                            clauses.append(recovered)
                            clause_end_idx.append(i)
                        clause, switch = [], False  # êµ¬ì ˆ ì´ˆê¸°í™”
                    clause.append(tok)  # í† í° ëˆ„ì 
                # ë§ˆì§€ë§‰ êµ¬ì ˆë„ ì²˜ë¦¬
                if clause:
                    clauses.append(recover_wordpieces(clause).strip('. '))
                    clause_end_idx.append(i + 1)

                while clauses : # ì ˆ ê¸¸ì´ ì œí•œ
                    c= clauses[0]
                    if len(c.split()) <= self.config.clause_len_threshold:
                        # print(f"Warning: Short clause detected: {c}")
                        clauses.remove(c)
                        continue
                    c= clauses[-1]
                    if len(c.split()) <= self.config.clause_len_threshold:
                        # print(f"Warning: Short clause detected: {c}")
                        clauses.remove(c)
                        continue
                    break

                if clauses:
                    video_sents.append(clauses)
                else:
                    video_sents.append([''])

                # í•´ë‹¹ ë¬¸ì¥ì˜ êµ¬ì ˆë³„ mean pooling ë²¡í„° ê³„ì‚°
                start, embeds = 0, []
                for i in clause_end_idx:
                    embeds.append(embeddings[start:i].mean(dim=0))  # [768]
                    start = i
                embedding_v.append(embeds)
            self.cls_vectors.append(cls_temp)
            results.append(video_sents)
            embedding_t.append(embedding_v)

        # ì…ë ¥ depthì— ë”°ë¼ ê²°ê³¼ í˜•íƒœ ì¡°ì •
        if depth == 1:
            return results[0], embedding_t[0]
        elif depth == 0:
            return results[0][0], embedding_t[0][0]
        return results, embedding_t

    def clause_embedding(self, splited, print_highlighted: bool = True, highlight: bool = True, sbert_option: bool = False):
        """
        ë¶„ë¦¬ëœ ì ˆ ë¦¬ìŠ¤íŠ¸(splited)ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
        - DeBERTa ì„ë² ë”©ì„ í†µí•´ ê° ì ˆì˜ [CLS] ì„ë² ë”© ì¶”ì¶œ
        - ê° í† í° ë²¡í„°ì™€ [CLS], tagging-model CLS, mean pooled ë²¡í„° ê°„ cosine similarityë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ ì¶”ì¶œ
        - embedding ë° ê°•ì¡° ê²°ê³¼ë¥¼ JSONLê³¼ npyë¡œ ì €ì¥

        Args:
            splited (List[List[List[str]]]): ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ [[video][sentence][clause]]
            print_highlighted (bool): ê°•ì¡° ë‹¨ì–´ ì¶œë ¥ ì—¬ë¶€ (ì½˜ì†” ë””ë²„ê¹…ìš©)

        Returns:
            List[List[List[np.ndarray]]]: ì ˆ ë‹¨ìœ„ [CLS] ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ì €ì¥ ì—¬ë¶€ì— ë”°ë¼ None ê°€ëŠ¥)

        Side effects:
            - ./saved_temp í´ë”ì— batchë³„ .npy ì €ì¥
            - significant_jsonl íŒŒì¼ì— clause + ê°•ì¡° ë‹¨ì–´ JSONL ì €ì¥
            - embedding_np íŒŒì¼ë¡œ ëª¨ë“  ì„ë² ë”© ë³‘í•© ì €ì¥
        """
        
        def delete_history_log(save_dir: str, file_type = None):
            """
            save_dir ë‚´ì— ì¡´ì¬í•˜ëŠ” {file_type}_batch_*.npy íŒŒì¼ë“¤ì„ ì°¾ì•„ ì‚­ì œí•©ë‹ˆë‹¤.

            Args:
                save_dir (str): ì €ì¥ëœ ë°°ì¹˜ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬
                output_path (str): ìµœì¢… ì¶œë ¥ ê²°ê³¼ ê²½ë¡œ (í•´ë‹¹ íŒŒì¼ë„ ì‚­ì œí•¨)
                file_type (str): íŒŒì¼ ì ‘ë‘ì–´ (ì˜ˆ: 'embedding', 'relation' ë“±)
            """
            if file_type is None:
                print("âš ï¸ file_typeì„ ì§€ì •í•˜ì„¸ìš”.")
                return

            files = [
                f for f in os.listdir(save_dir)
                if f.startswith(f"{file_type}_batch_") and f.endswith(".npy")]
            for file in files:
                full_path = os.path.join(save_dir, file)
                if os.path.exists(full_path):
                    os.remove(full_path)
            print(f"{len(files)}ê°œì˜ ì„ì‹œ íŒŒì¼ ì‚­ì œë¨.")


        def save_batch_npy(batch_result, save_dir, file, batch_idx):
            os.makedirs(save_dir, exist_ok=True)
            path = os.path.join(save_dir, f'{file}_batch_{batch_idx}.npy')
            np.save(path, np.array(batch_result, dtype=object))  # ì½ì„ë•Œ allow_pickle=True í•„ìš”


        with open(self.filenames.significant_jsonl, "w", encoding="utf-8") as f:
            pass  # ì´ˆê¸°í™”
        self.embedding_model = self.embedding_model.to("cuda")
        all_result = [] if len(splited) < self.config.return_embed_max else None

        print("splited.shape \t cls_vectors.shape    mp_embeds.shape")
        print(get_shape(splited), get_shape(self.cls_vectors), get_shape(self.meanpooled_embeds))
        delete_history_log(self.filenames.saved_temp_dir, file_type="embedding")
        delete_history_log(self.filenames.saved_temp_dir, file_type="sbert")
        # batch iter [3707,44,2,768] -> [37,100,44,2,768]
        for batch_idx in range(0, len(splited), self.config.save_batch):
            start = batch_idx
            end = batch_idx + self.config.save_batch
            batch = splited[start:end]
            batch_cls_vectors = self.cls_vectors[start:end] if len(self.cls_vectors) else None
            if not self.meanpooled_embeds == None:
                batch_meanpooled_embeds = self.meanpooled_embeds[start:end] 

            result, sbert, highlighted = [], [], [[],[],[]]
            # video iter : for [44,2,768] in [100,44,2,768]
            for V in tqdm(range(len(batch)), desc=f"Batch {batch_idx // self.config.save_batch}"):
                temp_embed, temp_sbert, highlight_temp = [], [], [[],[],[]] 
                # sentence iter : for [2,768] in [44,2,768]
                for S in range(len(batch[V])): 
                    temp_video, _temp_sbert, highlight_video = [], [], [[],[],[]]
                    # clause iter : for [768] in [2,768]
                    for C in range(len(batch[V][S])):
                        s = batch[V][S][C]
                        if batch_cls_vectors:
                            s_cls = batch_cls_vectors[V][S].to('cuda')
                        if self.meanpooled_embeds != None:
                            s_mp_emb = batch_meanpooled_embeds[V][S][C].to('cuda')

                        # êµ¬ë¬¸ ë‹¨ìœ„ [CLS] ì„ë² ë”© ì¶”ì¶œ
                        inputs = self.tokenizer(text=s, return_tensors='pt', add_special_tokens=True)
                        input_ids = inputs["input_ids"]
                        inputs = {k: v.to("cuda") for k, v in inputs.items()}
                        with torch.no_grad():
                            outputs = self.embedding_model(**inputs)
                        hidden_states, cls_clause = outputs.last_hidden_state, outputs.last_hidden_state[:, 0, :]
                        temp_video.append(cls_clause.squeeze(0).cpu().numpy())
                        if sbert_option:
                            _temp_sbert.append(self.sbert(cls_clause.squeeze(0).cpu(), hidden_states[0]).detach().cpu().numpy())

                        if highlight:
                            # ë‹¨ì–´ ìˆ˜ì¤€ ì˜ë¯¸ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì „ì²˜ë¦¬
                            real = self.str2real(s, output_str=False)
                            tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
                            token_map = []
                            for idx, tok in enumerate(tokens):
                                if tok in self.tokenizer.all_special_tokens:
                                    continue
                                clean_tok = tok[2:] if tok.startswith("##") else tok
                                for word in real:
                                    if clean_tok in word:
                                        token_map.append((word, idx))
                                        break

                            # ê° ë‹¨ì–´ë³„ í† í° ì¸ë±ìŠ¤ ì§‘ê³„
                            word2indices = {}
                            for word, idx in token_map:
                                word2indices.setdefault(word, []).append(idx)

                            # ê° ë‹¨ì–´ì˜ ë²¡í„°ë“¤ê³¼ [CLS] ë²¡í„° ê°„ cosine similarity í‰ê·  ê³„ì‚°
                            standard1 = cls_clause[0].unsqueeze(0)
                            standard2 = s_cls.unsqueeze(0) if batch_cls_vectors else None
                            standard3 = s_mp_emb.unsqueeze(0) if self.meanpooled_embeds else None

                            def similarity(standard):
                                if standard == None:
                                    return
                                word_scores = []
                                for word, indices in word2indices.items():
                                    vecs = torch.stack([hidden_states[0, i] for i in indices])

                                    sims = F.cosine_similarity(vecs, standard, dim=1)
                                    score = self.rms(sims)  
                                    word_scores.append((word, float(score)))
                                word_scores_sorted = sorted(word_scores, key=lambda x: x[1], reverse=True)                 

                                # ìœ ì‚¬ë„ ê¸°ì¤€ ìƒìœ„ ë‹¨ì–´ë¥¼ ì˜ë¯¸ ê°•ì¡° ë‹¨ì–´ë¡œ ì„ ì • (default 60%)
                                top_n = max(1, int(len(word_scores_sorted) * self.config.important_words_ratio))
                                top_words = {word for word, _ in word_scores_sorted[:top_n]}
                                return [word for word in real if word in top_words]

                            highlight_video[0].append(similarity(standard1))
                            highlight_video[1].append(similarity(standard2))
                            highlight_video[2].append(similarity(standard3))
                    
                    temp_embed.append(temp_video)
                    temp_sbert.append(_temp_sbert)
                    if highlight:
                        highlight_temp[0].append(highlight_video[0])
                        highlight_temp[1].append(highlight_video[1])
                        highlight_temp[2].append(highlight_video[2])
                result.append(temp_embed)
                sbert.append(temp_sbert)
                if highlight:
                    highlighted[0].append(highlight_temp[0])
                    highlighted[1].append(highlight_temp[1])
                    highlighted[2].append(highlight_temp[2])

            if print_highlighted and highlight:
                for sentences, h1,h2,h3 in zip(batch, highlighted[0],highlighted[1],highlighted[2]):
                    for a,b,c in zip(highlight(sentences, h1, True),highlight(sentences, h2, True),highlight(sentences, h3, True)):
                        print('C:',a)
                        print('S:',b)
                        if self.meanpooled_embeds != None:
                            print('E:',c)
                        print()
                    break
            if highlight:
                for clauses, highlights in zip(batch, highlighted[0]):
                    item = {"clause": clauses, "highlight": highlights}
                    with open(self.filenames.significant_jsonl, "a", encoding="utf-8") as f:
                        f.write(json.dumps(item, ensure_ascii=False) + "\n")

            save_batch_npy(result, self.filenames.saved_temp_dir, file="embedding", batch_idx=batch_idx)
            if sbert:
                save_batch_npy(sbert, self.filenames.saved_temp_dir, file="sbert", batch_idx=batch_idx)
            if all_result is not None:
                all_result.extend(result)

        def load_and_merge_npy(save_dir: str, output_path: str,file_type = None):
            files = [f for f in os.listdir(save_dir) if f.startswith(f"{file_type}_batch_") and f.endswith(".npy")]
            if not files:
                raise FileNotFoundError("ë³‘í•©í•  .npy íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))
            if len(files) == 1:
                src = os.path.join(save_dir, files[0])
                np.save(output_path, np.load(src, allow_pickle=True))
                return
            merged = []
            for file in files:
                batch_path = os.path.join(save_dir, file)
                data = np.load(batch_path, allow_pickle=True)
                merged.extend(data)
            np.save(output_path, np.array(merged, dtype=object))
            print(f"npy file merged! path: {output_path} and length is : ",len(merged))

        load_and_merge_npy(self.filenames.saved_temp_dir, self.filenames.embedding_np, file_type="embedding")
        if sbert:
            load_and_merge_npy(self.filenames.saved_temp_dir, self.filenames.sbert_np, file_type="sbert")
        return all_result

    def sbert(self, cls_clause, hidden_states, mode= 'mean'):
        projected = self.concat_project(cls_clause, hidden_states, mode=mode)
        return projected

    def is_gram(self, word):
        """ì£¼ì–´ì§„ ë‹¨ì–´ê°€ ì¡°ì‚¬(J), ì–´ë¯¸(E), ì ‘ë¯¸ì‚¬(XS)ì¸ì§€ ì—¬ë¶€ í™•ì¸"""
        t = self.kiwi.tokenize(word)[-1].tag
        return t[0] in ['J', 'E'] or t[:2] == 'XS'

    def is_segm(self, word, prev):
        """ë‘ í† í° ê²°í•© ì‹œ ì˜ë¯¸ ë‹¨ìœ„(N/V/M/XR ë“±)ë¡œ ë¶„ë¦¬ë  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨"""
        combined = prev + word.strip('#') if word.startswith('#') else prev + ' ' + word
        t = self.kiwi.tokenize(combined)[-1].tag
        return t[0] in ['N', 'V', 'M'] or t[:2] == 'XR'

    def rms(self, x: torch.Tensor) -> torch.Tensor:
        """Root Mean Square ì—°ì‚° (ìœ ì‚¬ë„ í‰ê·  ê³„ì‚° ì‹œ í™œìš©)"""
        return torch.sqrt(torch.mean(x ** 2))

    def str2real(self, text, timecat=True, output_str=True):
        """
        í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ìš”ì†Œë§Œ ì¶”ì¶œ
        timecat=Trueì¼ ê²½ìš° ì‹œê°„ ê´€ë ¨ ìˆ«ì ë¬¶ìŒë„ ì²˜ë¦¬í•¨
        """
        tokens = self.kiwi.tokenize(text)
        return ' '.join(self.bereal(tokens, timecat)) if output_str else self.bereal(tokens, timecat)

    def bereal(self, tokens, timecat=True):
        """
        í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì˜ë¯¸ ìˆëŠ” í˜•íƒœì†Œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        í•„í„°ë§ ë°©ì‹:
        - ì£¼ìš” í’ˆì‚¬(tag): ëª…ì‚¬, ë™ì‚¬, ìˆ«ì, ì™¸ë˜ì–´ ë“± take ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ê²½ìš°ë§Œ ìœ ì§€
        - ì‹œê°„ ê´€ë ¨ ìˆ«ì ë¬¶ìŒ(timecat=True): '2023ë…„ 1ì›”' ë“±ì€ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•© ì²˜ë¦¬

        Args:
            tokens (List[Token]): kiwi.tokenize() ê²°ê³¼
            timecat (bool): ì‹œê°„ ë¬¶ìŒ ì²˜ë¦¬ ì—¬ë¶€

        Returns:
            List[str]: ì˜ë¯¸ ê¸°ë°˜ ë‹¨ì–´ ì‹œí€€ìŠ¤
        """
        real, timeset = [], []
        take = ['NNG', 'NNP', 'NNB', 'NP', 'NR', 'XR', 'SN', 'SL', 'VV', 'VA', 'MM', 'MAJ', 'MAG']
        timeTrigger = ['ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ì„¸']
        for token in tokens:
            if token.tag in take:
                if not timecat:
                    real.append(token.form)
                    continue
                if token.tag in ['SN', 'NR']:
                    timeset.append(token.form)
                elif token.form in timeTrigger or token.tag == 'NNB':
                    if timeset:
                        timeset.append(token.form)
                elif len(timeset) > 1:
                    real.append(''.join(timeset))
                    timeset = []
                elif timeset:
                    real.append(timeset[0])
                    timeset = []
                real.append(token.form)
        return real

    def extract_tail_morphemes(self, eojeol: str) -> str:
        tokens = self.kiwi.tokenize(eojeol)
        tail_tags = {'EC', 'EF', 'ETN', 'ETM',  # ì–´ë¯¸
                    'JKS', 'JKC', 'JKG', 'JKB', 'JKV', 'JKQ', 'JX', 'JC',  # ì¡°ì‚¬
                    'XSN', 'XSV', 'XSA'}  # ì ‘ë¯¸ì‚¬
        collected = []

        for tok in reversed(tokens):
            if tok.tag in tail_tags:
                collected.insert(0, tok)
            else:
                break  # ì—°ì†ëœ ì–´ë¯¸/ì¡°ì‚¬/ì ‘ë¯¸ì‚¬ê°€ ëë‚˜ë©´ ì¤‘ë‹¨
        return self.kiwi.join(collected) if collected else ''

    def summary(self, sample: int = 0):
        """
        ë¶„ë¦¬ëœ ì ˆê³¼ í•´ë‹¹ ê°•ì¡° ë‹¨ì–´, [CLS] ì„ë² ë”© ë²¡í„° ì •ë³´ë¥¼ ìš”ì•½ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
        - ì¤‘ìš” ë‹¨ì–´ ê°•ì¡° ì—¬ë¶€, ì„ë² ë”© ê°œìˆ˜, ë¬¸ì¥/ì ˆ ìˆ˜ ë“± í†µê³„ ì œê³µ
        Args:
            max_sentence: ì¶œë ¥í•  ë¬¸ì¥ ìˆ˜ ì œí•œ (ê¸°ë³¸ê°’ 5)
        """
        try:
            # ë§Œì•½ ë©”ëª¨ë¦¬ì— ì—†ê³ , embedding_np íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œ ì‹œë„
            if not self.embeds:
                if os.path.exists(self.filenames.embedding_np):
                    self.embeds = np.load(self.filenames.embedding_np, allow_pickle=True)
                    print("[INFO] ì„ë² ë”© ë¡œë”© ì™„ë£Œ â†’ shape:", get_shape(self.embeds))
                else:
                    print("[ê²½ê³ ] embedding_np íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì¶œë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
        except Exception as e:
            print("[ì—ëŸ¬] ì„ë² ë”© ë³µì› ì‹¤íŒ¨:", e)
            return

        if sample > 0:
            print("\nğŸ“Œ ì ˆ ë¶„ë¦¬ ë° ê°•ì¡° ë‹¨ì–´ ì˜ˆì‹œ:")
            for a in highlight_jsonl(self.filenames.significant_jsonl, sample=sample):
                print(a)
                print()

        # ì ˆ, ë¬¸ì¥ ìˆ˜ ê´€ë ¨ í†µê³„
        total_sentences = sum(len(sentence) for sentence in self.splited)
        total_videos = len(self.splited)
        avg_sentences = total_sentences / total_videos if total_videos > 0 else 0
        avg_clauses = self.history['num_clauses'] / total_sentences if total_sentences > 0 else 0


        print("\nğŸ“Š [ë¶„ì„ ìš”ì•½]")
        print(f"âœ… ì „ì²´ ë¹„ë””ì˜¤ ìˆ˜          : {total_videos}")
        print(f"âœ… ì „ì²´ ë¬¸ì¥ ìˆ˜            : {total_sentences}")
        print(f"âœ… ì „ì²´ êµ¬ì ˆ ìˆ˜            : {self.history['num_clauses']}")
        print(f"âœ… ë¹„ë””ì˜¤ë‹¹ í‰ê·  ë¬¸ì¥ ìˆ˜   : {avg_sentences:.2f}")
        print(f"âœ… ë¬¸ì¥ë‹¹ í‰ê·  êµ¬ì ˆ ìˆ˜     : {avg_clauses:.2f}")
        print(f"âœ… ì¶”ì¶œëœ ê´€ê³„ (triplets)  : {self.history['num_triplets']}ê°œ")
        print("\nğŸ§¾ [ì¤‘ìš” ë³€ìˆ˜ êµ¬ì¡°]")
        print(f"ğŸ“Œ self.splited          : {get_shape(self.splited)} ({type(self.splited).__name__})")
        print(f"ğŸ“Œ self.meanpooled_embeds: {get_shape(self.meanpooled_embeds)}")
        print(f"ğŸ“Œ self.embeds           : {get_shape(self.embeds)}")
        print(f"ğŸ“Œ self.cls_vectors      : {get_shape(self.cls_vectors)}")
        print(f"ğŸ“Œ self.sentences        : {get_shape(self.sentences)}")

    def find_rel(self):
        """
        í˜•íƒœì†Œ ë¶„ì„ê¸°(kiwi)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¦¬ëœ ì ˆ ê°„ì˜ ì˜ë¯¸ ê´€ê³„(triplet)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ë‹¨ì„œ ìœ í˜•:
        - ì–´ë¯¸(aumi): ì ˆ ë§ˆì§€ë§‰ì—ì„œ ì°¾ìŒ
        - ì ‘ì†ì‚¬(conj): ì ˆ ì‹œì‘ì—ì„œ ì°¾ìŒ
        - ì–´ê°„(augan): í˜„ì¬ ë¯¸ì‚¬ìš©

        ê´€ê³„ ì²˜ë¦¬:
        - ê´€ê³„ê°€ ê²¹ì¹˜ëŠ” ê²½ìš° max(rel_id)ë¥¼ ì ìš©
        - ë‹¨ì„œ/ì—­í• /ê´€ê³„ ë¶„ë¥˜ëŠ” CSVì—ì„œ ë¶ˆëŸ¬ì˜¤ë©°, 'ì—†ìŒ'ì€ ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤.

        Side effects:
            - self.history['num_triplets']ì— ê´€ê³„ ìˆ˜ ì €ì¥
            - triplets_np ê²½ë¡œì— (clause_id1, clause_id2, relation_id) ì €ì¥
        """
        relation_trigger_ = pd.read_csv(self.filenames.relation_trigger)
        relation_trigger = relation_trigger_[['ë‹¨ì„œ', 'ì—­í• ', 'ìµœì¢…ë¶„ë¥˜']] # ë‹¨ì„œ, ì—­í• , ê´€ê³„ë¶„ë¥˜
        relation_set = set([s.strip() for s in set(relation_trigger['ìµœì¢…ë¶„ë¥˜'].unique())])
        aumi, conj, augan = [], [], []
        for _, row in relation_trigger.iterrows():
            pair = (row['ë‹¨ì„œ'].strip(), row['ìµœì¢…ë¶„ë¥˜'].strip())
            if row['ì—­í• '] == 'ì–´ë¯¸':
                aumi.append(pair)
            elif row['ì—­í• '] == 'ì ‘ì†ì‚¬':
                conj.append(pair)
            elif row['ì—­í• '] == 'ì–´ê°„':
                augan.append(pair)
            else:
                raise ValueError('We\'ve got wrong data')

        def aumi_exception(v, t_last): # Trueë©´ ê±¸ëŸ¬ì§
            term = v[0].strip('~')
            if term == 'ë‹ˆê¹Œ':
                for t in t_last:
                    if term in t.form:
                        if t.tag != 'EC':
                            return True
                        break
            elif term in ['í•˜ì—¬','í•´']: # 'í•˜ì—¬', 'í•´' ì²˜ë¦¬ 
                for t in t_last:
                    if t.form ==  'í•˜':
                        if t.tag != 'XSV':
                            return True
                    elif t.form == 'ì–´':
                        if t.tag != 'EC':
                            return True
            elif term == 'ë‹ˆ':
                for t in t_last:
                    if t.form == 'ë‹ˆ':
                        if t.tag != 'EC':
                            return True
            return False

        rel_map = {'ì—†ìŒ': 0}
        rel_map.update({label: i+1 for i, label in enumerate(sorted(relation_set))})
        # rel_map = {'ì—†ìŒ': 0, 'ê¸°íƒ€': 1, 'ëŒ€ì¡°/ë³‘ë ¬': 2, 'ìƒí™©': 3, 'ìˆ˜ë‹¨': 4, 'ì—­ì¸ê³¼': 5, 'ì˜ˆì‹œ': 6, 'ì¸ê³¼': 7}
        self.rel_map = rel_map  # save relation map for later use

        sentences = self.splited_id_mapping()
        print(f"ê´€ê³„ ì¶”ì¶œ ì‹œì‘: {get_shape(sentences)}ê°œì˜ ë¬¸ì¥, {len(self.splited)}ê°œì˜ ë¹„ë””ì˜¤ì—ì„œ ê´€ê³„ ì¶”ì¶œ")
        triplets = []    # video ë‹¨ìœ„ë¡œ ì•ˆ ë‚˜ëˆ”.
        print("Finding relations...")
        for sentence in tqdm(sentences):
            triplet_temp = {}
            for c_idx in range(len(sentence)):
                clause = sentence[c_idx][1] 
                rel = rel_map['ì—†ìŒ'] # initialize
                # aumi
                last = ' '.join(clause.split(', ')[-2:]).strip('. ') # ë§ˆì§€ë§‰ ë‘ ì–´ì ˆ
                t_last = self.kiwi.tokenize(last)
                for v in aumi:
                    if any(v[0].strip('~ ') in t.form for t in t_last):
                        if aumi_exception(v, t_last):
                            continue
                        rel = rel_map[v[1]]
                        break
                if c_idx < len(sentence) - 1:
                    triplet_temp[(sentence[c_idx][0], sentence[c_idx+1][0])] = rel # idë§Œ ì €ì¥
                # conj
                first = ' '.join(clause.split()[:2]).strip('. ') # ì²« ë‘ ì–´ì ˆ
                t_first = self.kiwi.tokenize(first)
                for v in conj:
                    if any(v[0].strip() in t.form for t in t_first):
                        rel = rel_map[v[1]]
                        break
                if c_idx > 0:
                    triplet_temp[(sentence[c_idx-1][0], sentence[c_idx][0])] = max(rel, triplet_temp.get((sentence[c_idx-1][0], sentence[c_idx][0]), 0))
            triplets.extend([(id1, id2, rel) for (id1, id2), rel in triplet_temp.items()])
        # save triplets
        triplets_np = np.array(triplets, dtype=np.int32)  # shape: [N, 3] (id, id, rel)
        np.save(self.filenames.triplets_np, triplets_np)
        self.history['num_triplets'] = len(triplets)
        print(f"ê´€ê³„ ì¶”ì¶œ ì™„ë£Œ: {self.history['num_triplets']}ê°œì˜ ê´€ê³„ ì¶”ì¶œë¨.")

    def set_db(self):
        """
        ë¶„ë¦¬ëœ ì ˆ(self.splited)ì„ clause_id ê¸°ë°˜ìœ¼ë¡œ DBì— ì €ì¥í•©ë‹ˆë‹¤.

        - clause_id = V*100000 + S*10 + C í˜•ì‹ìœ¼ë¡œ ìƒì„±
        - ë„ˆë¬´ ê¸´ S(ë¬¸ì¥ ì¸ë±ìŠ¤ > 9999)ë‚˜ C(ì ˆ ì¸ë±ìŠ¤ > 9)ëŠ” ì œì™¸
        - ì¼ì • ê°œìˆ˜ë§ˆë‹¤ ë°°ì¹˜ë¡œ insertí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ

        Side effects:
            - clause_data í…Œì´ë¸”ì— ì ˆ ë‚´ìš© ì €ì¥
            - self.history['num_clauses']ì— ì ˆ ìˆ˜ ì €ì¥
            - ClauseDB.close() í˜¸ì¶œí•˜ì—¬ DB ì¢…ë£Œ
        """
        db = ClauseDB(self.filenames.clause_db, self.filenames.embedding_np)
        batch = [] # batchsize: 1000ê°œ
        for V, video in enumerate(self.splited): # max ì—†ìŒ
            for S, sentence in enumerate(video): # max 10000
                if S >= 10000:
                    print("Eliminated VS:",V,S)
                    break # ì˜¤ë²„í•˜ë©´ ë²„ë¦¼
                for C, clause in enumerate(sentence): # max 10
                    if not isinstance(clause,str):
                        raise ValueError(f"Clause has a problem! : {clause}")
                    if C >= 10:
                        print("Eliminated VSC :",V,S,C)
                        break # ì˜¤ë²„í•˜ë©´ ë²„ë¦¼
                    clause_id = V*100000 + S*10 + C
                    batch.append((clause_id, clause))

                    if len(batch) >= 1000:  # batch insert
                        db.insert_batch(batch)
                        batch = []
        if batch:
            db.insert_batch(batch)
        self.history['num_clauses'] = db.count_clauses()
        db.close()

    def splited_id_mapping(self) -> list:
        """
        db: ClauseDB instance
        """
        db = ClauseDB(self.filenames.clause_db, self.filenames.embedding_np)
        result = db.get_all_clauses(return_format='sents', return_id=True)

        if len(result) != self.history['num_clauses']:
            print(f"Warning: Mismatch in clause count! Expected {self.history['num_clauses']}, got {len(result)}")
        db.close()
        return result

    def print_triplets(self, number = float('inf'), triplets: np.ndarray = None):
        """
        DBì— ì €ì¥ëœ ì ˆ(clause)ì„ ë°”íƒ•ìœ¼ë¡œ, ì €ì¥ëœ ê´€ê³„(triplets)ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

        ìƒ‰ìƒ:
        - ì—†ìŒ(0): íŒŒë‘
        - ì£¼ìš” ê´€ê³„(1~6): ë…¸ë‘
        - íŠ¹ìˆ˜ê´€ê³„(7): ë§ˆì  íƒ€

        Args:
            number (int): ì¶œë ¥í•  ìµœëŒ€ ê´€ê³„ ìˆ˜
            triplets (np.ndarray, optional): ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜¨ (id1, id2, rel_id) ë°°ì—´. ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ìë™ ë¡œë“œ
        """
        db = ClauseDB(self.filenames.clause_db, self.filenames.embedding_np, self.rel_map)
        if triplets is None:
            if not os.path.exists(self.filenames.triplets_np):
                print("No triplets found. Please run find_rel() first.")
                return
            triplets = np.load(self.filenames.triplets_np)
        i=0
        for id1, id2, rel_id in triplets:
            clause1 = db.get_clause(id1)
            clause2 = db.get_clause(id2)
            if rel_id == 7:
                color = ('\033[95m', '\033[0m')
            elif rel_id == 0:
                color = ('\033[94m', '\033[0m')
            else:
                color = ('\033[93m', '\033[0m')
            rel = db.rev_map.get(rel_id, 'ì—†ìŒ')
            if clause1 and clause2:
                print(f"{clause1}  -({color[0]}{rel}{color[1]})->  {clause2}")
            else:
                print(f"Invalid triplet: ({id1}:{clause1}, {id2}:{clause2}, {rel_id}:{rel})")
            if i >= number:
                break
            i += 1
        db.close()

class ClauseDB:
    """
    ì ˆ(clause) ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ê¸° ìœ„í•œ SQLite ê¸°ë°˜ ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤.

    ì£¼ìš” ì—­í• :
    - ê° ì ˆì„ ê³ ìœ  ID(clause_id)ë¡œ ì €ì¥í•˜ê³  ì¡°íšŒ ê°€ëŠ¥í•˜ê²Œ êµ¬ì„±
    - clause_id = V*100000 + S*10 + C ë¡œ êµ¬ì„±ë˜ì–´ ìœ„ì¹˜ ì •ë³´ë¥¼ ì••ì¶• í‘œí˜„
    - numpy ê¸°ë°˜ ì„ë² ë”© ë²¡í„°ë„ í•¨ê»˜ ê´€ë¦¬ (ë©”ëª¨ë¦¬ + íŒŒì¼ ì €ì¥ ê°€ëŠ¥)
    - ê´€ê³„ ID ë§¤í•‘(rel_map)ë„ í•¨ê»˜ ë³´ê´€í•˜ì—¬ triplet ì¶œë ¥ì— í™œìš©

    ì €ì¥ êµ¬ì¡°:
    - ì ˆ í…ìŠ¤íŠ¸: SQLiteì˜ clause_data í…Œì´ë¸” (id, clause)
    - ì ˆ ì„ë² ë”©: numpy íŒŒì¼(allow_pickle=True)ë¡œ [V][S][C][768] êµ¬ì¡° ì €ì¥

    Args:
        db_path (str): SQLite DB íŒŒì¼ ê²½ë¡œ
        embedding_path (str): ì„ë² ë”© ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°ìš© npy íŒŒì¼ ê²½ë¡œ
        rel_map (dict): ê´€ê³„ IDì™€ ëª…ì¹­ ê°„ ë§¤í•‘ ì •ë³´ (ex: {'ì—†ìŒ': 0, 'ì›ì¸': 1, ...})
    """
    
    def __init__(self, db_path, embedding_path, rel_map: dict = None):
        print("[ClauseDB] Opening DB:", db_path)
        self.rel_map = rel_map if rel_map else {'ì—†ìŒ': 0, 'ê¸°íƒ€': 1, 'ëŒ€ì¡°/ë³‘ë ¬': 2, 'ìƒí™©': 3, 'ìˆ˜ë‹¨': 4, 'ì—­ì¸ê³¼': 5, 'ì˜ˆì‹œ': 6, 'ì¸ê³¼': 7}
        self.rev_map = {v: k for k, v in rel_map.items()} if rel_map else {0: 'ì—†ìŒ'}
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        self.conn = sqlite3.connect(db_path)
        self.cur = self.conn.cursor()
        # self.cur.execute("PRAGMA journal_mode = WAL") # ì†ë„ í–¥ìƒ
        # self.cur.execute("PRAGMA synchronous = OFF") # ì†ë„ í–¥ìƒ but ì†ì‹¤ ê°€ëŠ¥
        self.embedding_path = embedding_path
        self._create_table()

        if os.path.exists(self.embedding_path):
            self.embeddings = list(np.load(self.embedding_path, allow_pickle=True))  # list of video
        else:
            self.embeddings = []

    def id2VSC(self, clause_id):
        clause_id = int(clause_id)
        V = clause_id // 100000
        S = (clause_id % 100000) // 10
        C = clause_id % 10
        return V,S,C

    def f5(self):
        if os.path.exists(self.embedding_path):
            self.embeddings = list(np.load(self.embedding_path, allow_pickle=True))
            return True
        else:
            return False

    def _create_table(self):
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS clause_data (
                id INTEGER PRIMARY KEY,
                clause TEXT NOT NULL
            )
        """)
        self.conn.commit()

    def insert_batch(self, batch: list[tuple[int, str]]):
        """
        ì ˆ í…ìŠ¤íŠ¸ë¥¼ idì™€ í•¨ê»˜ SQLite DBì— ì¼ê´„ ì‚½ì…í•©ë‹ˆë‹¤.

        Args:
            batch (List[Tuple[clause_id, clause]]): ì ˆ IDì™€ í…ìŠ¤íŠ¸ ìŒ ë¦¬ìŠ¤íŠ¸
        """
        if not batch:
            print("âš ï¸ [insert_batch] ë¹„ì–´ìˆëŠ” ë°°ì¹˜ê°€ ë“¤ì–´ì™”ìŠµë‹ˆë‹¤. ì²˜ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        valid_batch = []
        for i, item in enumerate(batch):
            try:
                int(item[0])
            except ValueError:
                print(f"âŒ [insert_batch] ì˜ëª»ëœ í˜•ì‹ @ index {i}: {item}")
            if (not isinstance(item, tuple) or
                len(item) != 2 or
                not isinstance(item[1], str)):
                print(f"âŒ [insert_batch] ì˜ëª»ëœ í˜•ì‹ @ index {i}: {item}")
                continue
            valid_batch.append(item)

        if not valid_batch:
            print("âŒ [insert_batch] ìœ íš¨í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ì‚½ì… ìƒëµ.")
            return

        try:
            self.cur.executemany(
                "INSERT OR REPLACE INTO clause_data (id, clause) VALUES (?, ?)",
                valid_batch
            )
            self.conn.commit()
            # print(f"âœ… [insert_batch] ì´ {len(valid_batch)}ê°œ ì‚½ì… ì™„ë£Œ.")
        except Exception as e:
            print(f"ğŸ”¥ [insert_batch] DB ì‚½ì… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

    def insert_video(self, video_embeddings: list[list], clause_items: list[tuple[int, str]], auto_save=True):  
        """
        í•˜ë‚˜ì˜ videoì— ëŒ€í•œ clause ì „ì²´ë¥¼ ì„ë² ë”©ê³¼ í•¨ê»˜ DBì— ì‚½ì…í•©ë‹ˆë‹¤.

        Args:
            video_embeddings (List[List[np.ndarray]]): ì ˆ ì„ë² ë”© [S][C][768]
            clause_items (List[Tuple[int, str]]): clause_idì™€ ì ˆ í…ìŠ¤íŠ¸ ìŒ
            auto_save (bool): Trueì¼ ê²½ìš° ì‚½ì… í›„ npyë¡œ ì„ë² ë”© ì €ì¥ ìˆ˜í–‰
        """
        print("check embedding shape:", get_shape(self.embeddings), end=' -> ')
        self.insert_batch(clause_items)
        self.embeddings.append(np.array(video_embeddings))  # shape: [S][C][768]
        print(get_shape(self.embeddings))
        if auto_save:
            self.save_embedding_files()
        else:
            print("You have to execute \"save_embedding_files()\" function to save!!")

    def save_embedding_files(self):
        """
        í˜„ì¬ê¹Œì§€ ë©”ëª¨ë¦¬ì— ìŒ“ì¸ ëª¨ë“  ì„ë² ë”© ë¦¬ìŠ¤íŠ¸(self.embeddings)ë¥¼ numpy íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        ì €ì¥ ì‹œ ragged ë°°ì—´ì´ë¯€ë¡œ dtype=objectì™€ allow_pickle=Trueë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        np.save(self.embedding_path, np.array(self.embeddings, dtype=object))  # ragged OK

    def get_clause(self, clause_id):
        """
        [clause_id] -> [clause] idì— ë§ëŠ” ì ˆ í…ìŠ¤íŠ¸ë¥¼ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤.
        """
        clause_id = str(clause_id)
        self.cur.execute("SELECT clause FROM clause_data WHERE id=?", (clause_id,))
        row = self.cur.fetchone()
        return row[0] if row else None

    def get_embedding(self, clause_id):
        """
        [clause_id] -> [embedding vector] idì— ë§ëŠ” ì„ë² ë”©ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        self.f5()
        V,S,C = self.id2VSC(clause_id)
        if V < len(self.embeddings) and S < len(self.embeddings[V]) and C < len(self.embeddings[V][S]):
            return self.embeddings[V][S][C]
        return None

    def get_all_embedding(self, return_id = False, return_dict = False):
        self.f5()
        flatten = {} if return_dict else []
        print("Embedding number : ",len([clause for video_unit in self.embeddings for sentence in video_unit for clause in sentence]))
        for V, video in tqdm(enumerate(self.embeddings), desc="Embedding ë¡œë”©"):
            if video is None: continue
            for S, sentence in enumerate(video):
                if sentence is None: continue
                for C, emb in enumerate(sentence):
                    if emb is None: continue
                    clause_id = V * 100000 + S * 10 + C
                    if return_dict:
                        flatten[clause_id] = emb
                    else :
                        flatten.append((clause_id,emb) if return_id else emb)
        return flatten 
        
    def get_id(self, clause_text):
        """
        !!! ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì£¼ì˜: ì ˆ í…ìŠ¤íŠ¸ëŠ” ê³ ìœ í•´ì•¼ í•©ë‹ˆë‹¤. !!!
        [clause_text] -> [clause_id] ì ˆ í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” idë¥¼ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            clause_text (str): ì ˆ í…ìŠ¤íŠ¸

        Returns:
            int ë˜ëŠ” None: í•´ë‹¹ í…ìŠ¤íŠ¸ì— ë§¤ì¹­ë˜ëŠ” id (ì—†ìœ¼ë©´ None)
        """
        self.cur.execute("SELECT id FROM clause_data WHERE clause=?", (clause_text,))
        row = self.cur.fetchone()
        return int(row[0]) if row else None

    def get_all_clauses(self, return_format="videos", return_id=False):
        """
        DBì— ì €ì¥ëœ ëª¨ë“  ì ˆì„ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            return_format (str): 'videos', 'sents', 'clauses' ì¤‘ ì„ íƒ
                - videos: [V][S][C] êµ¬ì¡°  / (id, clause)
                - sents: [V*S][C] êµ¬ì¡°    / (id, clause)
                - clauses: clause ë¦¬ìŠ¤íŠ¸  / {clause_id: clause} ë”•ì…”ë„ˆë¦¬
            return_id (bool): Falseì¼ ê²½ìš° id ì—†ì´ ì ˆ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜

        Returns:
            List or Dict: ì„ íƒëœ í˜•ì‹ì˜ ì ˆ ë°ì´í„°
        """
        if return_format not in ('videos','sents','clauses'):
            raise ValueError(f"Invalid return_format: '{return_format}'. Choose from \[videos, sents, clauses\]")
        
        self.cur.execute("SELECT id, clause FROM clause_data")
        rows = self.cur.fetchall()
        print(f"get_all_clauses : Fetched {len(rows)} clauses from the database.")

        if return_format == "clauses":
            return dict(rows) if return_id else [clause for _, clause in rows]
        
        video = []

        for clause_id, clause in rows:
            V,S,C = self.id2VSC(clause_id)
            while len(video) <= V: # V ì°¨ì› í™•ì¥
                video.append([])
            while len(video[V]) <= S: # S ì°¨ì› í™•ì¥
                video[V].append([])
            while len(video[V][S]) <= C: # C ì°¨ì› í™•ì¥
                video[V][S].append(None)

            video[V][S][C] = clause if not return_id else (clause_id, clause)
        if return_format == "videos":
            return video
        
        result = []
        for video_sents in video:
            for sentence in video_sents:
                result.append(sentence[:])
        return result
    
    def count_clauses(self):
        self.cur.execute("SELECT COUNT(*) FROM clause_data")
        length = self.cur.fetchone()[0]
        return length
    
    def close(self):
        self.conn.close()

    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_value, traceback):
        self.close()
        if exc_type is not None:
            print(f"[ClauseDB] Error occurred: {exc_value}")
        else:
            print("[ClauseDB] Closed successfully.")

    def reset_database(self):
        """
        clause_data í…Œì´ë¸”ì˜ ëª¨ë“  ë‚´ìš©ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        ì„ë² ë”©(npy íŒŒì¼)ì€ ìœ ì§€ë©ë‹ˆë‹¤.
        """
        self.cur.execute("DELETE FROM clause_data")
        self.conn.commit()
        print("[ClauseDB] clause_data í…Œì´ë¸”ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def reset_embeddings(self):
        """
        ë©”ëª¨ë¦¬ ë‚´ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ì™€ ì €ì¥ëœ .npy íŒŒì¼ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        SQLite DBëŠ” ìœ ì§€ë©ë‹ˆë‹¤.
        """
        self.embeddings = []
        if os.path.exists(self.embedding_path):
            os.remove(self.embedding_path)
            print(f"[ClauseDB] ì„ë² ë”© íŒŒì¼ ì‚­ì œë¨: {self.embedding_path}")
        else:
            print("[ClauseDB] ì‚­ì œí•  ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    def update_embedding(self, clause_id, embedding: np.ndarray):
        if len(self.embeddings) == 0:
            if not self.f5():
                raise FileNotFoundError("There's no embedding saved.")
        V,S,C = self.id2VSC(clause_id)
        try:
            self.embeddings[V][S][C] = embedding
        except IndexError as e:
            print('\033[95m'+"Embedding vector is not founded."+'\033[0m', e, f"[{V} {S} {C}]")
        self.save_embedding_files()
        self.f5()
    
    def delete_clause(self, clause_id: int):
        """
        clause_data í…Œì´ë¸”ì—ì„œ í•´ë‹¹ IDì˜ í…ìŠ¤íŠ¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        """
        clause_id = int(clause_id)
        self.cur.execute("DELETE FROM clause_data WHERE id=?", (clause_id,))
        self.conn.commit()

class ConcatProject(nn.Module):
    """
    SBERT ë°©ì‹ ë¬¸ì¥ ì„ë² ë”© ìƒì„±ê¸°
    - [CLS] + mean/max pooled vector â†’ concat â†’ projection
    """
    def __init__(self, input_size = 768):
        super(ConcatProject, self).__init__()
        self.device = 'cuda'
        self.linear = nn.Linear(input_size * 2, input_size).to(self.device)

    def forward(self, cls_vector, hidden_states, mode='mean'):
        cls_vector = cls_vector.to(self.device)
        if mode == 'mean':
            pooled = hidden_states[1:-1].mean(dim=0).to(self.device)  # [H]
        elif mode == 'max':
            pooled = hidden_states[1:-1].max(dim=0).values.to(self.device)  # [H]
        else:
            raise ValueError(f"Unsupported pooling mode: {mode}")

        if pooled.shape != cls_vector.shape:
            raise ValueError(f"Shape mismatch: pooled {pooled.shape}, cls_vector {cls_vector.shape}")
        
        concatted = torch.cat([cls_vector, pooled], dim=0)  # [2H]
        projected = self.linear(concatted)  # [H]
        return projected


def main():
    config = Config()
    config.confidence_threshold = 0.15 # êµ¬ë¬¸ ë¶„ë¦¬ ê¸°ì¤€, ë†’ì„ ìˆ˜ë¡ ë§ì´ ì˜ë¦¼ë¦¼
    config.important_words_ratio = 0.5 # ì¤‘ìš” í‚¤ì›Œë“œ ê¸°ì¤€, ë†’ì„ ìˆ˜ë¡ ë§ì´ íƒì§€
    config.clause_len_threshold = 3    # êµ¬ë¬¸ ê¸¸ì´ ì œí•œ, ì–´ì ˆ ë‹¨ìœ„ 

    dir_ = "./top_6.json"
    file = "./top_6_parsed.json"
    filtered_file = "./top_6_filtered.json"
    if os.path.exists(file):
        print("preprocessed file detected!")
        with open(file, "r", encoding="utf-8-sig") as f:
            sentences = json.load(f)
    else:
        sentences = select_terms(open_and_preprocess(dir_,file),filtered_file)

    sentences = sentences
    cs = ClauseSpliting(sentences, e_option= 'E3', threshold= True)
    cs.find_rel()
    cs.print_triplets(40)
    cs.summary(0)
    
if __name__ == "__main__":
    main()
