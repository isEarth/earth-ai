# ì „ì²´ Triplet ì¶”ì¶œ ë° í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# === ëª©ì  ===
# ì´ ì½”ë“œëŠ” ë¬¸ì¥ì„ ì ˆ(clause) ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ í›„, ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê´€ê³„(triplet)ë¥¼ ì¶”ì¶œí•˜ê³ ,
# ìœ ì‚¬í•œ ì ˆë“¤ì„ ë³‘í•©í•˜ë©° ì¤‘ë³µ ì œê±° ë° GNN í•™ìŠµ ë°ì´í„°ë¡œ ì •ì œí•˜ëŠ” end-to-end íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
# 
# === ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ===
# - Triplets: ì ˆ ê°„ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° ë° ê´€ê³„ ì¶”ë¡  (ìœ ì‚¬/ë°˜ëŒ€)
# - AfterProcess: ìœ ì‚¬ ì ˆ ë³‘í•©, triplet ID ì •ê·œí™”, ì¤‘ë³µ ì œê±°, ìµœì¢… ì €ì¥
# - ClauseDB: ì ˆê³¼ ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ëŠ” SQLite + numpy ê¸°ë°˜ DB
# - prepare_gnn: ì¸ê³¼ ì¤‘ì‹¬ì˜ tripletì„ edge ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ GNN í•™ìŠµì— ì í•©í•œ í¬ë§·ìœ¼ë¡œ ì €ì¥
# 
# === ì „ì²´ ì‹¤í–‰ íë¦„ ===
# 1. jsonì—ì„œ ì ˆ ì •ë³´ ë¡œë”© ë° ë¬´íš¨/ê³µë°± ì ˆ ì „ì²˜ë¦¬
# 2. ClauseDBì— ì„ë² ë”© ì €ì¥ (ê¸°ì¡´ íŒŒì¼ì´ ì—†ì„ ê²½ìš° SBERTë¡œ ì¬ìƒì„±)
# 3. cosine similarity ë° L2 distance ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ì ˆ ìŒ ì¶”ì¶œ
# 4. Triplets í´ë˜ìŠ¤ì—ì„œ ìœ ì‚¬/ë°˜ëŒ€ ê´€ê³„ë¡œ íƒœê¹… ë° ì €ì¥
# 5. AfterProcess í´ë˜ìŠ¤ì—ì„œ
#    - ì‚­ì œ ëŒ€ìƒ ë° ìœ ì‚¬ ID ì œê±°
#    - í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ë³‘í•© ë° ì„ë² ë”© í‰ê· í™”
#    - triplet ë‚´ ID ì¹˜í™˜ ë° ì •ì œ
#    - í…ìŠ¤íŠ¸ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
#    - ìƒˆë¡œìš´ triplet ì €ì¥
# 6. prepare_gnn í•¨ìˆ˜ì—ì„œ ì¸ê³¼/ìƒí™©/ì—­ì¸ê³¼ ê´€ê³„ë§Œ edgeë¡œ ì¶”ì¶œí•´ GNN ì…ë ¥ìœ¼ë¡œ ì €ì¥
# 7. ìµœì¢…ì ìœ¼ë¡œ ì •ì œëœ triplet ê°œìˆ˜ì™€ ê´€ê³„ ë¶„í¬ë¥¼ ì¶œë ¥í•˜ì—¬ í™•ì¸



from dataclasses import dataclass
import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from kiwipiepy import Kiwi
from collections import defaultdict
import numpy as np
from tqdm import tqdm
from train import Config
from prediction import ClauseSpliting, ClauseDB, FileNames, get_shape

filenames_db = FileNames()
@dataclass
class FilePaths:
    name: str = "_top6"
    db_dir: str = "./user_data/"                                  # DB ë° ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    db_path: str = f"{db_dir}user_clauses{name}.db"              # ì ˆ í…ìŠ¤íŠ¸ DB (SQLite)
    embedding_path: str = f"{db_dir}user_embeddings{name}.npy"   # ì ˆ ì„ë² ë”© ì €ì¥ íŒŒì¼ (npy, [V][S][C][768])
    relation_np :str = f"{db_dir}relation_sbert{name}.npy"        # ê´€ê³„ ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ (ì„ì‹œ ì €ì¥)
    similar_np :str = f"{db_dir}similar{name}.npy"                # ìœ ì‚¬í•œ ì ˆ ID ìŒ ë¦¬ìŠ¤íŠ¸ (List[Tuple[str, str]])
    opposite_np :str = f"{db_dir}opposite{name}.npy"              # ë°˜ëŒ€ ì˜ë¯¸ ì ˆ ID ìŒ ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
    similar_temp_np :str = f"{db_dir}similar_temp{name}.npy"      # ìœ ì‚¬ìŒ ì¤‘ê°„ ì €ì¥ íŒŒì¼ (ì˜µì…˜)
    new_triplet_np : str = f"{db_dir}new_triplet{name}.npy"       # í›„ì²˜ë¦¬ëœ ìµœì¢… triplet (np.ndarray[(id1, id2, rel_id)])
    final_relation_triplets_np : str = f"{db_dir}final_relation_triplets{name}.npy"  # GNNìš© edge (ì›ì¸-ê²°ê³¼)
    no_duplicated_triplets_np : str = f"{db_dir}no_duplicated_triplets{name}.npy"    # í…ìŠ¤íŠ¸ ê¸°ì¤€ ì¤‘ë³µ ì œê±°ëœ triplet
    similar_cluster_np: str = f"{db_dir}similar_cluster{name}.npy"  # ìœ ì‚¬ ì ˆ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìºì‹œ

    temp_dir : str = f"./saved_temp/"                             # ì„ì‹œ ì¤‘ê°„ ì €ì¥ í´ë”

    saved_triplets_np: str = ""                                   # ì´ˆê¸° triplet ì €ì¥ íŒŒì¼
    def __post_init__(self):
        global filenames_db
        self.saved_triplets_np = filenames_db.triplets_np.replace("./", "../youtube_clause/")

filepaths = FilePaths()
config = Config()



class Triplets():
    """
    Triplets í´ë˜ìŠ¤ëŠ” ì…ë ¥ ì ˆ(clause)ì— ëŒ€í•´ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - ë¬´íš¨ ì ˆ í•„í„°ë§
    - ì„ë² ë”© ìƒì„± ë° DB ì €ì¥
    - ì ˆ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ë° ê´€ê³„ ë¶„ë¥˜ ('ìœ ì‚¬', 'ë°˜ëŒ€', 'ë¬´ê´€')

    Attributes:
        deleted (List[str]): ì œê±°ëœ clause ID ëª©ë¡
        similar_twins (List[Tuple[str, str]]): ìœ ì‚¬ ê´€ê³„ë¡œ ë¶„ë¥˜ëœ ID ìŒ
        opposite_twins (List[Tuple[str, str]]): ë°˜ëŒ€ ê´€ê³„ë¡œ ë¶„ë¥˜ëœ ID ìŒ
        filepaths (FilePaths): ê´€ë ¨ ê²½ë¡œ ì •ë³´ë¥¼ ë‹´ì€ ê°ì²´
    """
    def __init__(self):
        self.deleted = []
        self.similar_twins = []
        self.opposite_twins = []
        self.filepaths = FilePaths()

    def preprocessing(self,clauses: dict):
        """
        ì˜ë¯¸ ì—†ëŠ” ì ˆì„ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            clauses (dict): {id: clause_text} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬

        Returns:
            dict: í•„í„°ë§ëœ clause ë”•ì…”ë„ˆë¦¬
        """
        to_delete = []
        for cid, text in clauses.items():
            if not isinstance(text, str) or not text or text.isspace() or text == "ì…ë ¥í•˜ì‹  ë¬¸ì¥ì„ ì œê³µí•´ ì£¼ì„¸ìš”.":
                to_delete.append(cid)
        
        for cid in to_delete:
            del clauses[cid]
        
        self.deleted = to_delete
        return clauses

    @staticmethod
    def infer_relation(sim: float, dist: float, sim_thresh=0.9, dist_thresh=0.1):
        """
        cosine similarityì™€ L2 distanceë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ê³„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.

        Args:
            sim (float): cosine similarity (-1 ~ 1 ì •ê·œí™”ë¨)
            dist (float): L2 ê±°ë¦¬
            sim_thresh (float): ìœ ì‚¬ ê¸°ì¤€ threshold
            dist_thresh (float): ë°˜ëŒ€ ê¸°ì¤€ threshold

        Returns:
            str: 'ìœ ì‚¬', 'ë°˜ëŒ€', ë˜ëŠ” 'ë¬´ê´€'
        """
        if sim > sim_thresh:
            return "ìœ ì‚¬"  # ì˜ë¯¸ ìœ ì‚¬
        elif sim < 0.3 and dist < dist_thresh:
            return "ë°˜ëŒ€"  # ë‹¨ì–´ê°€ ë¹„ìŠ·í•œë°, ì˜ë¯¸ê°€ ë‹¤ë¥´ë©´ â†’ ë°˜ëŒ€
        else:
            return "ë¬´ê´€"  # ì˜ë¯¸ë„ ë‹¤ë¥´ê³  ë‹¨ì–´ë„ ë‹¤ë¦„

    def infer_relation_pair(self, pair_list, clauses_dict, sim_thresh=0.97, dist_thresh=0.6, print_rel = False):
        """
        ì €ì¥ëœ ìœ ì‚¬ë„ ê²°ê³¼(pair_list)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 'ìœ ì‚¬', 'ë°˜ëŒ€' ê´€ê³„ë¥¼ ì¶”ë¡ .
        
        Args:
            pair_list (List[Tuple[str, str, float, float]]): (id1, id2, cos_sim, l2_dist)
            clauses_dict (dict): {id: clause_text}
            sim_thresh (float): ìœ ì‚¬ ê¸°ì¤€ threshold
            dist_thresh (float): ë°˜ëŒ€ ê¸°ì¤€ threshold
            print_rel (bool): ê´€ê³„ ì¶œë ¥ ì—¬ë¶€

        Returns:
            Saves:
                - filepaths.similar_np
                - filepaths.opposite_np
            Sets:
                - self.similar_twins
                - self.opposite_twins
        """
        similar_twins = []
        opposite_twins = []

        for id1, id2, cos_sim, l2_dist in tqdm(pair_list, desc="ê´€ê³„ ì¶”ë¡  ì¤‘"):
            norm_sim = (cos_sim + 1) / 2             # [-1, 1] â†’ [0, 1]
            norm_dist = 1 - (1 / (1 + l2_dist))       # [0, âˆ) â†’ [0, 1)

            rel = Triplets.infer_relation(norm_sim, norm_dist, sim_thresh, dist_thresh)

            if clauses_dict.get(id1, '').strip() == '':
                continue

            if rel in ["ìœ ì‚¬", "ë°˜ëŒ€"] and print_rel:
                print(f"[{rel}] {clauses_dict[id1]}  ///  {clauses_dict[id2]}  (sim: {cos_sim:.3f})")

            if rel == 'ìœ ì‚¬':
                similar_twins.append((id1, id2))
            elif rel == 'ë°˜ëŒ€':
                opposite_twins.append((id1, id2))

        # ê²°ê³¼ ì €ì¥ ë° ì „ì—­ êµ¬ì¡° ì—…ë°ì´íŠ¸
        self.similar_twins = similar_twins
        self.opposite_twins = opposite_twins

        np.save(filepaths.similar_np, np.array(similar_twins))
        np.save(filepaths.opposite_np, np.array(opposite_twins))

    def set_embedding(self, db, clauses, batch_size:int= 1000):
        """
        ì ˆ IDì™€ í…ìŠ¤íŠ¸ë¥¼ SBERTë¥¼ í†µí•´ ì„ë² ë”©í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            db (ClauseDB): ì ˆê³¼ ì„ë² ë”©ì„ ì €ì¥í•˜ëŠ” DB ì¸ìŠ¤í„´ìŠ¤
            clauses (List[Tuple[str, str]]): [(id, clause)] ë¦¬ìŠ¤íŠ¸
            batch_size (int): DB ì €ì¥ ë°°ì¹˜ í¬ê¸°

        Returns:
            None
        """
        # sbert ì²˜ë¦¬ëœ embedding ì‚¬ìš© 
        print("clause shape : ",get_shape(clauses))

        video = []
        for clause_id, clause in clauses:
            clause_id = int(clause_id)
            V = clause_id // 100000
            S = (clause_id % 100000) // 10
            C = clause_id % 10
            while len(video) <= V: # V ì°¨ì› í™•ì¥
                video.append([])
            while len(video[V]) <= S: # S ì°¨ì› í™•ì¥
                video[V].append([])
            while len(video[V][S]) <= C: # C ì°¨ì› í™•ì¥
                video[V][S].append(None)
            video[V][S][C] = clause
        print("clause number  :",len([clause for video_unit in video for sentence in video_unit for clause in sentence]))
        
        cs = ClauseSpliting(config=config, filenames=filenames_db, reference_mode=True)

        cs.clause_embedding(video, highlight=False, sbert_option = True)

        for batch in (clauses[i:i+batch_size] for i in range(0, len(clauses), batch_size)):
            db.insert_batch(batch)
        print(len(db.get_all_embedding()), len(db.get_all_clauses(return_format="clauses")))

    def is_same_pair(self, db, clause_ids, threshold=0.9, max_pair=10000, device='cuda'):
        """
        ì„ë² ë”© ë²¡í„° ê°„ cosine similarity ë° L2 distanceë¥¼ ê³„ì‚°í•˜ì—¬ ìœ ì‚¬ìŒ í›„ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            db (ClauseDB): ì„ë² ë”© ì •ë³´ë¥¼ í¬í•¨í•œ DB
            clause_ids (List[str]): clause ID ë¦¬ìŠ¤íŠ¸
            threshold (float): ìœ ì‚¬ ê¸°ì¤€ threshold (cosine)
            max_pair (int): ìµœëŒ€ ìœ ì‚¬ìŒ ê°œìˆ˜
            device (str): 'cuda' ë˜ëŠ” 'cpu'

        Returns:
            List[Tuple[str, str, float, float]]: ìœ ì‚¬ìŒê³¼ ê±°ë¦¬ ì •ë³´
        """
        proj = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Linear(256, 64)
        ).to(device)
        batch_size = 5000
        
        raw_list = db.get_all_embedding(return_id = True)[:max_pair] # sampling
        _clause_ids, emb_list = zip(*raw_list)
        dif = list(set(clause_ids) - set(_clause_ids))
        for dif_id in dif:
            print(dif_id,db.get_clause(dif_id))

        print("clause_ids, emb_list are ready!")
        print("db_clause : ",db.count_clauses())
        print("raw_list : ",len(raw_list))

        emb_list = [torch.tensor(emb, dtype=torch.float32) for emb in emb_list]
        embeddings = torch.stack(emb_list).to("cuda")

        print("Start fast_similarity : ",len(embeddings))
        N = len(embeddings)
        with tqdm(total=N*(N-1)//2, desc="cosine ì¶”ì¶œ") as pbar:
            with torch.no_grad():  # ì¶”ë¡ ìš©
                projected = proj(embeddings)  # [N, 64]

                #  cosine similarity matrix ê³„ì‚°
                normed = F.normalize(projected, dim=1)  # [N, D]
                fast_similarty = []
                for i in range(0, N, batch_size):
                    end_i = min(i + batch_size, N)
                    batch_i = normed[i:end_i]

                    for j in range(i, N, batch_size):
                        if j < i:
                            continue
                        end_j = min(j + batch_size, N)
                        batch_j = normed[j:end_j]

                        sim_block = torch.matmul(batch_i, batch_j.T)

                        if i == j:
                            for ii in range(end_i - i):
                                for jj in range(ii + 1, end_j - j):  # ìƒì‚¼ê°ë§Œ
                                    global_i = i + ii
                                    global_j = j + jj
                                    sim = sim_block[ii, jj].item()
                                    if sim > threshold:
                                        fast_similarty.append((global_i, global_j))
                                    pbar.update(1)
                        else:
                            for ii in range(end_i - i):
                                for jj in range(end_j - j):
                                    global_i = i + ii
                                    global_j = j + jj
                                    sim = sim_block[ii, jj].item()
                                    if sim > threshold:
                                        fast_similarty.append((global_i, global_j))
                                    pbar.update(1)

        final_results = []
        batch_size = 10000

        for k in tqdm(range(0, len(fast_similarty), batch_size), desc="ì •í™• ìœ ì‚¬ë„ ê³„ì‚°"):
            batch = fast_similarty[k:k+batch_size]

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…ì„œ êµ¬ì„±
            vec1_batch = torch.stack([embeddings[i] for i, j in batch])
            vec2_batch = torch.stack([embeddings[j] for i, j in batch])

            # GPU ì—°ì‚°
            cos_sim = F.cosine_similarity(vec1_batch, vec2_batch, dim=1)  # [B]
            eucl_dist = F.pairwise_distance(vec1_batch, vec2_batch, p=2)  # [B]

            # ê²°ê³¼ ê²°í•©
            for n in range(len(batch)):
                i, j = batch[n]
                final_results.append((_clause_ids[i], _clause_ids[j], cos_sim[n].item(), eucl_dist[n].item()))

        np.save(filepaths.similar_temp_np, np.array(final_results, dtype=object))
        print(f"[ì™„ë£Œ] ì €ì¥ ê²½ë¡œ: {filepaths.similar_temp_np} / ì´ ìœ ì‚¬ ìŒ ìˆ˜: {len(final_results)}")
        return final_results

    def group_connected(self, edges=None):
        """
        ìœ ì‚¬í•œ IDë“¤ ê°„ì˜ ì—°ê²° ê´€ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸(ê·¸ë£¹)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        Union-Find(Disjoint Set) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬, ì„œë¡œ ì§ì ‘ í˜¹ì€ ê°„ì ‘ì ìœ¼ë¡œ ì—°ê²°ëœ IDë“¤ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìŠµë‹ˆë‹¤.

        Args:
            edges (List[Tuple[str, str]], optional): ì—°ê²° ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìŒì˜ ë¦¬ìŠ¤íŠ¸.
                ì˜ˆ: [("A", "B"), ("B", "C")] â†’ "A", "B", "C"ëŠ” í•˜ë‚˜ì˜ ê·¸ë£¹
                ê¸°ë³¸ê°’ì€ self.similar_twins

        Returns:
            List[Set[str]]: ì—°ê²°ëœ ê·¸ë£¹ë“¤ì˜ ë¦¬ìŠ¤íŠ¸. ê° ê·¸ë£¹ì€ set í˜•íƒœë¡œ IDë¥¼ í¬í•¨.
                ì˜ˆ: [{"A", "B", "C"}, {"D", "E"}]
        """
        if not edges:
            edges = self.similar_twins
        parent = {}

        def find(x):
            # ê²½ë¡œ ì••ì¶•
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]

        def union(x, y):
            x_root = find(x)
            y_root = find(y)
            if x_root != y_root:
                parent[y_root] = x_root

        # ì´ˆê¸° parent ì„¤ì •
        nodes = set([n for pair in edges for n in pair])
        for node in nodes:
            parent[node] = node

        # ì—°ê²° ì •ë³´ ë°˜ì˜
        for a, b in edges:
            union(a, b)

        # ê·¸ë£¹í™”
        groups = defaultdict(set)
        for node in nodes:
            root = find(node)
            groups[root].add(node)

        return list(groups.values())


def delete_all_created_files():
    """
    ë¯¸ë¦¬ ì •ì˜ëœ ì„ì‹œ íŒŒì¼ë“¤ì„ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤.
    ì™¸ë¶€ ì¸ìˆ˜ ì—†ì´ í˜¸ì¶œë§Œìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤.
    """
    paths_to_delete = [
        filepaths.db_path,
        filenames_db.sbert_np,
        filepaths.similar_np,
        filepaths.opposite_np,
        filepaths.similar_temp_np,
        filepaths.final_relation_triplets_np,
        filepaths.no_duplicated_triplets_np,
    ]

    for path in paths_to_delete:
        if os.path.exists(path):
            os.remove(path)
            print(f"âœ… ì‚­ì œë¨: {path}")
        else:
            print(f"âš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {path}")

def concat_saved_batches(save_dir, output_path='final_results.npy'):
    """
    ì €ì¥ëœ pair_batch_*.npy íŒŒì¼ë“¤ì„ ë³‘í•©í•˜ì—¬ í•˜ë‚˜ì˜ NPYë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        save_dir (str): ì €ì¥ëœ ë°°ì¹˜ íŒŒì¼ ê²½ë¡œ
        output_path (str): ë³‘í•© í›„ ì €ì¥ ê²½ë¡œ

    Returns:
        np.ndarray: ë³‘í•©ëœ (id1, id2, cos_sim, l2_dist) ë°°ì—´
    """
    all_data = []
    files = sorted(f for f in os.listdir(save_dir) if f.startswith("pair_batch_") and f.endswith(".npy"))
    for fname in tqdm(files, desc="ë³‘í•© ì¤‘"):
        batch = np.load(os.path.join(save_dir, fname), allow_pickle=True)
        all_data.extend(batch)

    final = np.array(all_data, dtype=object)
    np.save(output_path, final)
    print(f"ìµœì¢… ë³‘í•© ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
    return final

class AfterProcess():
    """
    AfterProcess í´ë˜ìŠ¤ëŠ” triplet ì •ë³´ë¥¼ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤:
    - ìœ ì‚¬/ì‚­ì œëœ ID ì œê±°
    - ì„ë² ë”© ë³‘í•© ë° triplet ID í†µì¼
    - í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ë³‘í•© (ëŒ€í˜• ìœ ì‚¬ ê·¸ë£¹)
    - ì¤‘ë³µ ì œê±° ë° ì €ì¥

    Attributes:
        tls (Triplets): ê³µìœ ëœ Triplets ì¸ìŠ¤í„´ìŠ¤ (deleted, similar_twins ì°¸ì¡°)
        triplets_np (np.ndarray): (id1, id2, rel_id) triplet ë¦¬ìŠ¤íŠ¸
        embeddings (Dict[str, np.ndarray]): IDë³„ ì„ë² ë”© ë²¡í„°
        clause_dict (Dict[str, str]): IDë³„ ì ˆ í…ìŠ¤íŠ¸
    """
    def __init__(self, tls:Triplets, triplet_file, clause_dict):
        """
        Args:
            tls (Triplets): Triplets í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (deleted/similar_twins ì°¸ì¡°ìš©)
            triplet_file (str): ì›ë³¸ triplet npy íŒŒì¼ ê²½ë¡œ
            clause_dict (dict): {id: ì ˆ í…ìŠ¤íŠ¸} ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
        """
        self.tls = tls
        self.triplets_np = np.load(triplet_file, allow_pickle=True)
        self.new_triplet_file = filepaths.new_triplet_np
        self.shrinked = []
        self.similar_data = np.load(filepaths.similar_np, allow_pickle=True)
        self.db = ClauseDB(filepaths.db_path, filenames_db.sbert_np)
        print("DB êº¼ë‚´ê¸° ì‹œì‘.")
        self.embeddings = self.db.get_all_embedding(return_dict=True)
        self.clause_dict = clause_dict

        # êµ°ì§‘ ê¸°ë°˜ ë³‘í•© ìˆ˜í–‰
        # í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ë³‘í•© ìˆ˜í–‰
        clusters = self.cluster_large_group(threshold=20)
        for member_ids in clusters.values():
            base_id = member_ids[0]
            self.merge_cluster_embeddings(base_id, member_ids)
        self.after_process()
        self.db.close()

    def cluster_large_group(self, threshold=1000):
        """
        TF-IDF + KMeansë¡œ ìœ ì‚¬ ì ˆ ê·¸ë£¹ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ê³  ê²°ê³¼ë¥¼ ìºì‹±.

        Args:
            threshold (int): í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•  ìµœì†Œ ì ˆ ìˆ˜ ê¸°ì¤€

        Returns:
            dict: label â†’ [ì ˆ ID ëª©ë¡] í˜•íƒœì˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        if os.path.exists(filepaths.similar_cluster_np):
            print("ğŸ”„ ìºì‹œëœ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ë¡œë”© ì¤‘...")
            return np.load(filepaths.similar_cluster_np, allow_pickle=True).item()  # dict í˜•íƒœ

        id_texts = [(id1, self.clause_dict[id1]) for id1, _ in self.similar_data]
        id_texts = list({id: text for id, text in id_texts}.items())  # ì¤‘ë³µ ì œê±°

        if len(id_texts) < threshold:
            return {i: [id] for i, (id, _) in enumerate(id_texts)}  # ê·¸ëŒ€ë¡œ ë°˜í™˜

        ids, texts = zip(*id_texts)
        vectorizer = TfidfVectorizer()
        tfidf = vectorizer.fit_transform(texts)

        k = int(len(texts) / 100) + 1  # ëŒ€ëµ 100ê°œ ë‹¨ìœ„ í´ëŸ¬ìŠ¤í„°
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(tfidf)

        clusters = defaultdict(list)
        for idx, label in enumerate(labels):
            clusters[label].append(ids[idx])

        np.save(filepaths.similar_cluster_np, dict(clusters))  # ìºì‹± ì €ì¥
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìºì‹œ ì €ì¥: {filepaths.similar_cluster_np}")
        return clusters

    def merge_cluster_embeddings(self, base_id, member_ids):
        """ 
        í´ëŸ¬ìŠ¤í„° ë‚´ ì„ë² ë”© ë³‘í•© ë° triplet ID ì¼ê´„ ì¹˜í™˜.

        Args:
            base_id (str): ê¸°ì¤€ ID (ëŒ€í‘œ ID)
            member_ids (List[str]): í´ëŸ¬ìŠ¤í„° ë‚´ í¬í•¨ëœ ID ë¦¬ìŠ¤íŠ¸
        """
        base_emb = self.embeddings[base_id]
        for mid in member_ids:
            if mid == base_id:
                continue
            if mid not in self.embeddings:
                continue
            base_emb = (base_emb + self.embeddings[mid]) / 2
            self.update_triplets_with_clusters(mid, base_id)
            self.shrinked.append(mid)
        self.db.update_embedding(base_id, base_emb)

    def update_triplets_with_clusters(self, old_id, new_id):
        """
        triplet ë‚´ IDë¥¼ êµ°ì§‘ ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸.

        Args:
            old_id (str): ë³‘í•© ëŒ€ìƒ ID
            new_id (str): ê¸°ì¤€ ID (ë³‘í•© í›„ ëŒ€ì²´ ID)
        """
        for i in range(len(self.triplets_np)):
            h, t, r = self.triplets_np[i]
            if h == old_id:
                h = new_id
            if t == old_id:
                t = new_id
            self.triplets_np[i] = (h, t, r)

    def after_process(self):
        """
        Triplet ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‚­ì œ/ìœ ì‚¬ IDë¥¼ ì œê±°í•˜ê³ , ì¤‘ë³µ ê´€ê³„ë¥¼ í•„í„°ë§í•˜ì—¬ ì •ì œëœ tripletì„ ì €ì¥í•©ë‹ˆë‹¤.
        ì´í›„ í…ìŠ¤íŠ¸ ê¸°ì¤€ ì¤‘ë³µë„ ì œê±°í•˜ê³  ìš”ì•½ ì¶œë ¥ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        new_triplets = []
        for triplet in self.triplets_np:
            A, B, R = triplet
            if A in self.tls.deleted or B in self.tls.deleted:
                continue
            elif (A, B) in self.tls.similar_twins or (B, A) in self.tls.similar_twins:
                continue
            else:
                new_triplets.append(triplet)
        self.triplets_np = new_triplets

        dedup = []
        seen = set()
        for triplet in self.triplets_np:
            id1, id2, rel = triplet
            key = (id1, id2)
            if key not in seen:
                seen.add(key)
                dedup.append(triplet)
        self.triplets_np = dedup
        np.save(self.new_triplet_file, np.array(self.triplets_np, dtype=object))
        print(f"ìµœì¢… triplet ìˆ˜: {len(self.triplets_np)}ê°œ")
        self.earse_duplication_n_print(self.new_triplet_file, self.clause_dict, self.shrinked)

    def earse_duplication_n_print(self, triplet_file, clause_dict, shrinked=None, number = 100):
        """
        í…ìŠ¤íŠ¸ ê¸°ì¤€ ì¤‘ë³µ triplet ì œê±° í›„, ì¼ë¶€ ìƒ˜í”Œ ì¶œë ¥ ë° ì €ì¥.

        Args:
            triplet_file (str): triplet npy íŒŒì¼ ê²½ë¡œ
            clause_dict (dict): {id: ì ˆ í…ìŠ¤íŠ¸} ë§¤í•‘
            shrinked (List[str], optional): ë³‘í•©ëœ ID ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš© X)
            number (int): ì¶œë ¥í•  triplet ìƒ˜í”Œ ìˆ˜
        """
        rel_map = {'ì—†ìŒ': 0, 'ê¸°íƒ€': 1, 'ëŒ€ì¡°/ë³‘ë ¬': 2, 'ìƒí™©': 3, 'ìˆ˜ë‹¨': 4, 'ì—­ì¸ê³¼': 5, 'ì˜ˆì‹œ': 6, 'ì¸ê³¼': 7}
        rev_map = {v: k for k, v in rel_map.items()}
        triplets = np.load(triplet_file,allow_pickle=True)
        i=0
        no_duplicated = []
        seen = set()
        for id1, id2, rel_id in triplets:
            clause1 = clause_dict[int(id1)]
            clause2 = clause_dict[int(id2)] 
            if (clause1,clause2) in seen:
                continue
            seen.add((clause1,clause2))
            no_duplicated.append((id1,id2,rel_id))
            if rel_id == 7:
                color = ('\033[95m', '\033[0m')
            elif rel_id == 0:
                color = ('\033[94m', '\033[0m')
            else:
                color = ('\033[93m', '\033[0m')
            rel = rev_map.get(rel_id, 'ì—†ìŒ')
            if clause1 and clause2:
                print(f"{clause1}  -({color[0]}{rel}{color[1]})->  {clause2}")
            else:
                print(f"Invalid triplet: ({id1}:{clause1}, {id2}:{clause2}, {rel_id}:{rel})")
            if i >= number:
                break
            i += 1
        np.save(filepaths.no_duplicated_triplets_np,np.array(no_duplicated, dtype=object))


def prepare_gnn(triplets, save_file):
    """
    triplet ê´€ê³„ë¥¼ GNN í•™ìŠµì„ ìœ„í•œ edge arrayë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        triplets (np.ndarray): shape (N, 3) - (id1, id2, rel_id)
        save_file (str): ì €ì¥ ê²½ë¡œ (.npy)

    Returns:
        None
    """
    before_node = []
    after_node = []
    relation = []
    for id1,id2,rel in triplets:
        if rel == 0: # ê´€ê³„ì—†ìŒ
            continue
        elif rel == 5: # ì—­ì¸ê³¼
            before_node.append(id2)
            after_node.append(id1)
            relation.append(1)
        elif rel in [3,4,7]: # ì¸ê³¼, ìƒí™©, ìˆ˜ë‹¨
            before_node.append(id1)
            after_node.append(id2)
            relation.append(1)
        else: # ê¸°íƒ€ ê´€ê³„ ìˆìŒ
            before_node.append(id1)
            after_node.append(id2)
            relation.append(0)

    edge_array = np.stack([before_node, after_node, relation], axis=0)
    np.save(save_file, edge_array)
        
def solve_duplication_triplets(triplet_file):
    """
    triplet íŒŒì¼ì—ì„œ (id1, id2) ê¸°ì¤€ ì¤‘ë³µ ê´€ê³„ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

    Args:
        triplet_file (str): triplet .npy íŒŒì¼ ê²½ë¡œ

    Returns:
        None
    """
    # íŒŒì¼ ë¡œë“œ
    triplets = np.load(triplet_file, allow_pickle=True)

    # ì¤‘ë³µ ì œê±°: (id1, id2) ê¸°ì¤€ìœ¼ë¡œë§Œ ìœ ì¼í•˜ê²Œ ë‚¨ê¸°ê¸°
    seen = set()
    dedup_triplets = []
    for t in triplets:
        id1, id2, rel_id = t
        key = (id1, id2)
        if key not in seen:
            seen.add(key)
            dedup_triplets.append((id1, id2, rel_id))

    # ì €ì¥
    np.save(triplet_file, np.array(dedup_triplets, dtype=object))
    print(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(triplets)} â†’ {len(dedup_triplets)}ê°œë¡œ ì¤„ì—ˆìŠµë‹ˆë‹¤.")

def check_triplets(triplet_file):
    """
    Triplet íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ê°œìˆ˜, ê´€ê³„ ë¶„í¬, ìƒ˜í”Œì„ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        triplet_file (str): npy í˜•ì‹ì˜ triplet ì €ì¥ íŒŒì¼ ê²½ë¡œ
    """

    triplets = np.load(triplet_file,allow_pickle=True)
    print("Triplets number :", len(triplets))

    # ê³ ìœ  ê´€ê³„ íƒ€ì… ê°œìˆ˜
    rel_types = {}
    for _, _, r in triplets:
        rel_types[r] = rel_types.get(r, 0) + 1
    print("ğŸ“Š ê´€ê³„ ë¶„í¬:")
    for rel_id, count in sorted(rel_types.items()):
        print(f"  - rel_id={rel_id}: {count}ê°œ")

    # ìƒ˜í”Œ ëª‡ ê°œ ì¶œë ¥
    print("\nğŸ” Triplet ìƒ˜í”Œ:")
    for i, (h, t, r) in enumerate(triplets[:2]):
        print(f"  {i+1}. ({h}) --[{r}]--> ({t})")



def main():
    #  ê¸°ì¡´ triplet ê²°ê³¼ ê°„ë‹¨ í™•ì¸ (ìºì‹± ì—¬ë¶€ í™•ì¸ìš©)
    check_triplets(filepaths.saved_triplets_np)

    #  ê¸°ì¡´ íŒŒì¼ ì œê±° (ì¤‘ê°„ ê²°ê³¼ë“¤ ì´ˆê¸°í™”)
    json_path = 'clause_gpt_top6.json'
    delete_all_created_files()

    #  1. ì ˆ ë¡œë”© ë° ì „ì²˜ë¦¬
    with open(json_path, 'r', encoding='utf-8-sig') as f:
        clauses = json.load(f)
    triplets = Triplets()
    _clauses = triplets.preprocessing(clauses)
    _clauses_list = [(id, clause) for id, clause in _clauses.items()]

    print("we are watching", filepaths.db_path, filenames_db.sbert_np)

    #  2. Clause DB ë° ì„ë² ë”© ì²˜ë¦¬
    with ClauseDB(filepaths.db_path, filenames_db.sbert_np) as db:
        # ì„ë² ë”© íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if not os.path.exists(filenames_db.sbert_np):
            triplets.set_embedding(db, _clauses_list)

        #  3. DBë¡œë¶€í„° ì ˆ ì •ë³´ ë¡œë”©
        clauses_dict = db.get_all_clauses(return_format='clauses', return_id=True)
        clause_ids = list(clauses_dict.keys())
        print("clauses_dict loaded!", len(clause_ids))

        #  4. ìœ ì‚¬ìŒ ì¶”ì¶œ ë° ê´€ê³„ ì˜ˆì¸¡
        print("Pairing Started!!")
        if not os.path.exists(filepaths.similar_np):
            pair_list = triplets.is_same_pair(db, clause_ids)
            triplets.infer_relation_pair(pair_list, clauses_dict)
        print("----------------")

    #  5. Triplet í›„ì²˜ë¦¬ (ì¤‘ë³µ/ë³‘í•©/êµ°ì§‘í™” í¬í•¨)
    triplet_file = filepaths.saved_triplets_np
    after = AfterProcess(tls=triplets, triplet_file=triplet_file, clause_dict=clauses_dict)

    #  6. GNN í•™ìŠµìš© edge ì €ì¥
    prepare_gnn(np.load(filepaths.new_triplet_np, allow_pickle=True), filepaths.final_relation_triplets_np)

    #  7. ìµœì¢… Triplet ê²°ê³¼ í™•ì¸
    check_triplets(filepaths.new_triplet_np)
    check_triplets(filepaths.no_duplicated_triplets_np)
    print("<<<THE END>>>")

if __name__ == "__main__":
    main()
